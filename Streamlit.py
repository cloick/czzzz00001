import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import io
import base64
import tempfile
import os
import warnings
warnings.filterwarnings('ignore')

# Configuration de la page
st.set_page_config(
    page_title="Analyse des Enqu√™tes de Satisfaction",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Style CSS personnalis√©
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.8rem;
        color: #0277BD;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .section-header {
        font-size: 1.4rem;
        color: #0288D1;
        margin-top: 1.5rem;
        margin-bottom: 0.8rem;
    }
    .highlight {
        background-color: #E3F2FD;
        padding: 1rem;
        border-radius: 0.5rem;
    }
    .card {
        background-color: #f8f9fa;
        border-radius: 0.5rem;
        padding: 1.5rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin-bottom: 1.5rem;
    }
    .footer {
        text-align: center;
        color: #757575;
        font-size: 0.8rem;
        margin-top: 3rem;
    }
</style>
""", unsafe_allow_html=True)

# Fonction pour le pr√©traitement des textes
def preprocess_text_simple(text):
    if not isinstance(text, str):
        return ""
    
    # Conversion en minuscules
    text = text.lower()
    
    # Suppression des caract√®res sp√©ciaux et des chiffres
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', ' ', text)
    
    # Tokenisation simple par espace
    tokens = text.split()
    
    # Suppression des mots tr√®s courts
    tokens = [token for token in tokens if len(token) > 2]
    
    # Liste manuelle des mots vides fran√ßais courants
    stopwords_fr = ["les", "des", "est", "dans", "pour", "par", "pas", "avec", "sont", "ont", 
                   "mais", "comme", "tout", "plus", "autre", "autres", "nous", "vous", "ils", 
                   "elles", "leur", "cette", "ces", "notre", "nos", "votre", "vos", "elle", 
                   "ils", "elles", "nous", "vous", "leur", "leurs", "mon", "ton", "son", "mes", 
                   "tes", "ses", "qui", "que", "quoi", "dont", "o√π", "quand", "comment", 
                   "pourquoi", "lequel", "auquel", "duquel", "une", "deux", "trois", "quatre", 
                   "cinq", "six", "sept", "huit", "neuf", "dix", "√©t√©", "√™tre", "avoir", "fait", 
                   "faire", "dit", "dire", "cela", "ceci", "celui", "celle", "ceux", "celles", 
                   "tr√®s", "peu", "beaucoup", "trop", "bien", "mal", "tous", "toutes", "tout", 
                   "toute", "rien", "chaque", "plusieurs", "certains", "certaines", "m√™me", "aux", 
                   "sur", "sous", "entre", "vers", "chez", "sans", "avant", "apr√®s", "pendant", 
                   "depuis", "jusqu", "contre", "malgr√©", "sauf", "hors", "selon", "ainsi", "alors", 
                   "aussi", "donc", "puis", "ensuite", "enfin", "encore", "toujours", "jamais", 
                   "souvent", "parfois"]
    
    # Filtrage des mots vides
    tokens = [token for token in tokens if token not in stopwords_fr]
    
    return ' '.join(tokens)

# Fonction pour extraire les n-grammes les plus fr√©quents
def extract_ngrams(corpus, n_gram_range=(1, 3), top_n=20):
    vectorizer = CountVectorizer(ngram_range=n_gram_range)
    X = vectorizer.fit_transform(corpus)
    
    # Extraction des noms des n-grammes
    features = vectorizer.get_feature_names_out()
    
    # Somme des occurrences pour chaque n-gramme
    sums = X.sum(axis=0).A1
    
    # Cr√©ation d'un dictionnaire {n-gramme: nombre d'occurrences}
    ngrams_counts = {features[i]: sums[i] for i in range(len(features))}
    
    # Tri par fr√©quence d√©croissante et s√©lection des top_n
    top_ngrams = dict(sorted(ngrams_counts.items(), key=lambda x: x[1], reverse=True)[:top_n])
    
    return top_ngrams

# Fonction pour cr√©er un nuage de mots
def generate_wordcloud(text):
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        max_words=100,
        contour_width=1,
        contour_color='steelblue',
        collocations=False
    ).generate(text)
    
    return wordcloud

# Fonction pour la mod√©lisation th√©matique
def create_topic_model(df, n_topics=5, max_features=1000):
    # Vecteurisation des textes
    vectorizer = TfidfVectorizer(
        max_df=0.95,
        min_df=2,
        max_features=max_features,
        ngram_range=(1, 2)
    )
    
    # Transformation des textes en matrice TF-IDF
    X = vectorizer.fit_transform(df['verbatim_clean'])
    
    # Extraction des noms des features
    feature_names = vectorizer.get_feature_names_out()
    
    # Cr√©ation et entra√Ænement du mod√®le LDA
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        max_iter=20,
        learning_method='online',
        random_state=42,
        n_jobs=1
    )
    
    lda.fit(X)
    
    # Transformation des documents en distribution de th√®mes
    doc_topic_distrib = lda.transform(X)
    
    # Attribution du th√®me dominant √† chaque document
    df_with_topics = df.copy()
    df_with_topics['dominant_topic'] = doc_topic_distrib.argmax(axis=1) + 1
    
    # Extraction des termes les plus importants pour chaque th√®me
    topics = {}
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[:-11:-1]  # Top 10 mots
        top_words = [feature_names[i] for i in top_words_idx]
        topics[f"Th√®me {topic_idx+1}"] = ", ".join(top_words)
    
    return df_with_topics, topics, X, vectorizer, lda

# Fonction pour t√©l√©charger le DataFrame
def get_table_download_link(df, filename, link_text):
    """G√©n√®re un lien pour t√©l√©charger le dataframe en Excel"""
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False)
    b64 = base64.b64encode(output.getvalue()).decode()
    href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{filename}">{link_text}</a>'
    return href

# Fonction pour cr√©er un tableau de bord des probl√©matiques r√©currentes
def create_dashboard(df_topics, topics):
    theme_summary = pd.DataFrame()
    
    for theme_id in sorted(df_topics['dominant_topic'].unique()):
        theme_docs = df_topics[df_topics['dominant_topic'] == theme_id]
        
        # Top mots-cl√©s pour ce th√®me
        theme_key_words = topics[f"Th√®me {theme_id}"]
        
        # Exemples de verbatims pour ce th√®me
        verbatim_examples = theme_docs['Verbatim'].sample(min(3, len(theme_docs))).tolist()
        
        # Statistiques par PP/Direction CA-TS
        perimetre_counts = theme_docs['PP/Direction CA-TS'].value_counts().to_dict()
        top_perimetre = max(perimetre_counts.items(), key=lambda x: x[1])[0] if perimetre_counts else "N/A"
        
        # Informations sur les lignes sources
        lignes_sources = theme_docs['ligne_source'].tolist()
        
        # Construire l'entr√©e pour ce th√®me
        theme_entry = {
            'Th√®me ID': theme_id,
            'Mots-cl√©s': theme_key_words,
            'Nombre de verbatims': len(theme_docs),
            'PP/Direction principale': top_perimetre,
            'Exemples de verbatims': verbatim_examples,
            'Num√©ros de lignes sources': lignes_sources
        }
        
        # Ajouter √† notre r√©sum√©
        theme_summary = pd.concat([theme_summary, pd.DataFrame([theme_entry])], ignore_index=True)
    
    return theme_summary

# Fonction principale pour l'analyse des donn√©es
def analyze_data(df):
    # 1. Exploration initiale
    st.markdown('<div class="sub-header">1. Exploration initiale des donn√©es</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown('<div class="section-header">Aper√ßu des donn√©es</div>', unsafe_allow_html=True)
        st.dataframe(df.head())
    
    with col2:
        st.markdown('<div class="section-header">Informations sur le dataset</div>', unsafe_allow_html=True)
        st.markdown(f"""
        <div class="highlight">
            <p>Nombre d'entr√©es : {df.shape[0]}</p>
            <p>Nombre de colonnes : {df.shape[1]}</p>
        </div>
        """, unsafe_allow_html=True)
    
    # 2. Distribution des humeurs
    st.markdown('<div class="section-header">Distribution des humeurs</div>', unsafe_allow_html=True)
    
    humeur_counts = df['Humeur'].value_counts()
    
    fig = px.bar(
        x=humeur_counts.index,
        y=humeur_counts.values,
        labels={'x': 'Humeur', 'y': 'Nombre de r√©ponses'},
        title='Distribution des humeurs',
        color=humeur_counts.values,
        color_continuous_scale='Viridis'
    )
    
    fig.update_layout(
        xaxis={'categoryorder': 'total descending'},
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # 3. Filtrage des donn√©es n√©gatives
    st.markdown('<div class="sub-header">2. Analyse des retours n√©gatifs</div>', unsafe_allow_html=True)
    
    # Permettre √† l'utilisateur de s√©lectionner les valeurs n√©gatives
    all_humeurs = df['Humeur'].unique()
    negative_humeurs = st.multiselect(
        "S√©lectionnez les humeurs √† consid√©rer comme n√©gatives",
        all_humeurs,
        default=[h for h in all_humeurs if 'insatisfait' in h.lower() or 'n√©gativ' in h.lower()]
    )
    
    df_negatif = df[df['Humeur'].isin(negative_humeurs)]
    
    st.markdown(f"""
    <div class="highlight">
        <p>Nombre d'entr√©es n√©gatives s√©lectionn√©es : {df_negatif.shape[0]}</p>
        <p>Pourcentage du total : {(df_negatif.shape[0] / df.shape[0] * 100):.1f}%</p>
    </div>
    """, unsafe_allow_html=True)
    
    # 4. Distribution des PP/Direction CA-TS dans les retours n√©gatifs
    if not df_negatif.empty and 'PP/Direction CA-TS' in df_negatif.columns:
        st.markdown('<div class="section-header">PP/Direction CA-TS avec le plus de retours n√©gatifs</div>', unsafe_allow_html=True)
        
        perimetre_counts = df_negatif['PP/Direction CA-TS'].value_counts()
        
        fig = px.bar(
            x=perimetre_counts.index,
            y=perimetre_counts.values,
            labels={'x': 'PP/Direction CA-TS', 'y': 'Nombre de retours n√©gatifs'},
            title='Distribution des retours n√©gatifs par PP/Direction CA-TS',
            color=perimetre_counts.values,
            color_continuous_scale='Reds'
        )
        
        fig.update_layout(
            xaxis={'categoryorder': 'total descending', 'tickangle': 45},
            height=600
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # 5. Statut de traitement
    if not df_negatif.empty and 'Statut' in df_negatif.columns:
        st.markdown('<div class="section-header">Statut de traitement des retours n√©gatifs</div>', unsafe_allow_html=True)
        
        statut_counts = df_negatif['Statut'].value_counts()
        
        fig = px.pie(
            values=statut_counts.values,
            names=statut_counts.index,
            title='R√©partition des statuts de traitement',
            color_discrete_sequence=px.colors.sequential.Plasma_r
        )
        
        fig.update_traces(textposition='inside', textinfo='percent+label')
        
        st.plotly_chart(fig, use_container_width=True)
    
    # 6. Analyse temporelle des retours n√©gatifs
    if not df_negatif.empty and 'Date Enqu√™te' in df_negatif.columns:
        st.markdown('<div class="section-header">√âvolution temporelle des retours n√©gatifs</div>', unsafe_allow_html=True)
        
        # Conversion en datetime si n√©cessaire
        if not pd.api.types.is_datetime64_any_dtype(df_negatif['Date Enqu√™te']):
            df_negatif['Date Enqu√™te'] = pd.to_datetime(df_negatif['Date Enqu√™te'], errors='coerce')
        
        # Agr√©gation par mois
        df_negatif['mois'] = df_negatif['Date Enqu√™te'].dt.to_period('M').astype(str)
        trend_data = df_negatif.groupby('mois').size().reset_index(name='count')
        
        fig = px.line(
            trend_data, 
            x='mois', 
            y='count',
            markers=True,
            labels={'mois': 'Mois', 'count': 'Nombre de retours n√©gatifs'},
            title='√âvolution temporelle des retours n√©gatifs'
        )
        
        fig.update_layout(
            xaxis={'tickangle': 45},
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # 7. Comparaison des humeurs par p√©rim√®tre
    if 'PP/Direction CA-TS' in df.columns:
        st.markdown('<div class="section-header">Distribution des humeurs par PP/Direction CA-TS</div>', unsafe_allow_html=True)
        
        humeur_perim = pd.crosstab(df['PP/Direction CA-TS'], df['Humeur'])
        humeur_perim_pct = humeur_perim.div(humeur_perim.sum(axis=1), axis=0) * 100
        
        # Conversion en format long pour plotly
        humeur_perim_pct_long = humeur_perim_pct.reset_index().melt(
            id_vars=['PP/Direction CA-TS'],
            var_name='Humeur',
            value_name='Pourcentage'
        )
        
        fig = px.bar(
            humeur_perim_pct_long,
            x='PP/Direction CA-TS',
            y='Pourcentage',
            color='Humeur',
            title='Distribution des humeurs par PP/Direction CA-TS (%)',
            labels={'PP/Direction CA-TS': 'PP/Direction CA-TS', 'Pourcentage': 'Pourcentage (%)'}
        )
        
        fig.update_layout(
            xaxis={'categoryorder': 'total descending', 'tickangle': 45},
            height=600,
            barmode='stack'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # 8. Pr√©traitement des textes pour l'analyse
    if not df_negatif.empty and 'Verbatim' in df_negatif.columns:
        st.markdown('<div class="sub-header">3. Analyse des verbatims</div>', unsafe_allow_html=True)
        
        # Application du pr√©traitement aux verbatims n√©gatifs
        df_negatif['verbatim_clean'] = df_negatif['Verbatim'].apply(preprocess_text_simple)
        
        # Visualisation des exemples de verbatims nettoy√©s
        st.markdown('<div class="section-header">Exemples de verbatims nettoy√©s</div>', unsafe_allow_html=True)
        
        examples = pd.DataFrame({
            'Original': df_negatif['Verbatim'].head(5),
            'Nettoy√©': df_negatif['verbatim_clean'].head(5)
        })
        
        st.dataframe(examples)
        
        # 9. Visualisation des mots les plus fr√©quents
        st.markdown('<div class="section-header">Mots les plus fr√©quents dans les verbatims n√©gatifs</div>', unsafe_allow_html=True)
        
        all_words = ' '.join(df_negatif['verbatim_clean'].dropna()).split()
        word_counts = Counter(all_words)
        top_words = dict(word_counts.most_common(20))
        
        fig = px.bar(
            x=list(top_words.values()),
            y=list(top_words.keys()),
            orientation='h',
            labels={'x': 'Nombre d\'occurrences', 'y': 'Mot'},
            title='Top 20 des mots les plus fr√©quents',
            color=list(top_words.values()),
            color_continuous_scale='Viridis'
        )
        
        fig.update_layout(
            yaxis={'categoryorder': 'total ascending'},
            height=600
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # 10. Nuage de mots
        st.markdown('<div class="section-header">Nuage de mots des verbatims n√©gatifs</div>', unsafe_allow_html=True)
        
        wordcloud = generate_wordcloud(' '.join(df_negatif['verbatim_clean'].dropna()))
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        
        st.pyplot(fig)
        
        # 11. Analyse des n-grammes fr√©quents
        st.markdown('<div class="sub-header">4. Analyse des expressions r√©currentes</div>', unsafe_allow_html=True)
        
        # Extraction des n-grammes fr√©quents
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown('<div class="section-header">Bigrammes les plus fr√©quents</div>', unsafe_allow_html=True)
            top_bigrams = extract_ngrams(df_negatif['verbatim_clean'].dropna(), (2, 2), 15)
            
            fig = px.bar(
                x=list(top_bigrams.values()),
                y=list(top_bigrams.keys()),
                orientation='h',
                labels={'x': 'Nombre d\'occurrences', 'y': 'Bigramme'},
                color=list(top_bigrams.values()),
                color_continuous_scale='Blues'
            )
            
            fig.update_layout(
                yaxis={'categoryorder': 'total ascending'},
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.markdown('<div class="section-header">Trigrammes les plus fr√©quents</div>', unsafe_allow_html=True)
            top_trigrams = extract_ngrams(df_negatif['verbatim_clean'].dropna(), (3, 3), 15)
            
            fig = px.bar(
                x=list(top_trigrams.values()),
                y=list(top_trigrams.keys()),
                orientation='h',
                labels={'x': 'Nombre d\'occurrences', 'y': 'Trigramme'},
                color=list(top_trigrams.values()),
                color_continuous_scale='Greens'
            )
            
            fig.update_layout(
                yaxis={'categoryorder': 'total ascending'},
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        # 12. Mod√©lisation th√©matique
        st.markdown('<div class="sub-header">5. Mod√©lisation th√©matique</div>', unsafe_allow_html=True)
        
        # Filtrage des documents valides pour la mod√©lisation
        valid_docs = df_negatif['verbatim_clean'].dropna().replace('', np.nan).dropna()
        
        if len(valid_docs) < 5:
            st.warning("Nombre insuffisant de documents valides pour la mod√©lisation th√©matique.")
        else:
            # Param√®tres de mod√©lisation
            col1, col2 = st.columns(2)
            
            with col1:
                n_topics = st.slider(
                    "Nombre de th√®mes √† identifier",
                    min_value=2,
                    max_value=min(10, len(valid_docs) // 5),
                    value=min(5, len(valid_docs) // 5),
                    help="Nombre de th√®mes √† identifier dans les verbatims"
                )
            
            with col2:
                max_features = st.slider(
                    "Nombre maximum de features (mots)",
                    min_value=100,
                    max_value=2000,
                    value=1000,
                    step=100,
                    help="Nombre maximum de mots uniques √† consid√©rer"
                )
            
            # Cr√©ation du mod√®le
            df_negatif_topics, topics, X, vectorizer, lda = create_topic_model(
                df_negatif.loc[valid_docs.index],
                n_topics,
                max_features
            )
            
            # Affichage des th√®mes identifi√©s
            st.markdown('<div class="section-header">Th√®mes identifi√©s avec les termes les plus importants</div>', unsafe_allow_html=True)
            
            topics_df = pd.DataFrame({
                'Th√®me': list(topics.keys()),
                'Mots-cl√©s': list(topics.values())
            })
            
            st.dataframe(topics_df)
            
            # Distribution des documents par th√®me
            st.markdown('<div class="section-header">Distribution des documents par th√®me</div>', unsafe_allow_html=True)
            
            topic_counts = df_negatif_topics['dominant_topic'].value_counts().sort_index()
            
            fig = px.bar(
                x=topic_counts.index,
                y=topic_counts.values,
                labels={'x': 'Th√®me', 'y': 'Nombre de documents'},
                title='Nombre de verbatims par th√®me',
                color=topic_counts.values,
                color_continuous_scale='Viridis'
            )
            
            fig.update_layout(height=500)
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Distribution des th√®mes par PP/Direction CA-TS
            if 'PP/Direction CA-TS' in df_negatif_topics.columns:
                st.markdown('<div class="section-header">Distribution des th√®mes par PP/Direction CA-TS</div>', unsafe_allow_html=True)
                
                theme_perimetre = pd.crosstab(df_negatif_topics['dominant_topic'], df_negatif_topics['PP/Direction CA-TS'])
                theme_perimetre_pct = theme_perimetre.div(theme_perimetre.sum(axis=0), axis=1) * 100
                
                # Conversion en format long pour plotly
                theme_perimetre_pct_long = theme_perimetre_pct.reset_index().melt(
                    id_vars=['dominant_topic'],
                    var_name='PP/Direction CA-TS',
                    value_name='Pourcentage'
                )
                
                fig = px.bar(
                    theme_perimetre_pct_long,
                    x='dominant_topic',
                    y='Pourcentage',
                    color='PP/Direction CA-TS',
                    title='Distribution des th√®mes par PP/Direction CA-TS (%)',
                    labels={'dominant_topic': 'Th√®me', 'Pourcentage': 'Pourcentage (%)'}
                )
                
                fig.update_layout(
                    xaxis={'title': 'Th√®me dominant'},
                    height=600,
                    barmode='stack'
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            # Exemples de verbatims par th√®me
            st.markdown('<div class="section-header">Exemples de verbatims par th√®me</div>', unsafe_allow_html=True)
            
            for theme in sorted(df_negatif_topics['dominant_topic'].unique()):
                theme_docs = df_negatif_topics[df_negatif_topics['dominant_topic'] == theme]
                
                st.markdown(f"""
                <div class="card">
                    <h4>Th√®me {int(theme)} - {topics[f'Th√®me {theme}']} ({len(theme_docs)} documents)</h4>
                    <hr>
                """, unsafe_allow_html=True)
                
                for i, (idx, row) in enumerate(theme_docs.sample(min(3, len(theme_docs))).iterrows(), 1):
                    st.markdown(f"""
                    <p><strong>Exemple {i}:</strong> {row['Verbatim']}</p>
                    <p><small>PP/Direction CA-TS: {row['PP/Direction CA-TS'] if 'PP/Direction CA-TS' in row else 'N/A'}</small></p>
                    <hr>
                    """, unsafe_allow_html=True)
                
                st.markdown('</div>', unsafe_allow_html=True)
            
            # 13. Tableau de bord des probl√©matiques r√©currentes
            st.markdown('<div class="sub-header">6. Tableau de bord des probl√©matiques r√©currentes</div>', unsafe_allow_html=True)
            
            theme_summary = create_dashboard(df_negatif_topics, topics)
            
            # Affichage du r√©sum√©
            st.dataframe(theme_summary[['Th√®me ID', 'Mots-cl√©s', 'Nombre de verbatims', 'PP/Direction principale']])
            
            # Lien de t√©l√©chargement du tableau de bord
            st.markdown(get_table_download_link(theme_summary, 'dashboard_problematiques_recurrentes.xlsx', 'T√©l√©charger le tableau de bord complet (Excel)'), unsafe_allow_html=True)
            
            # 14. Tableau d√©taill√© des verbatims par th√®me
            detailed_df = df_negatif_topics[['ligne_source', 'dominant_topic', 'Verbatim', 'PP/Direction CA-TS', 'Humeur']].copy()
            detailed_df['Mots-cl√©s du th√®me'] = detailed_df['dominant_topic'].apply(lambda x: topics[f'Th√®me {x}'])
            
            st.markdown('<div class="section-header">Tableau d√©taill√© des verbatims par th√®me</div>', unsafe_allow_html=True)
            st.dataframe(detailed_df)
            
            # Lien de t√©l√©chargement du tableau d√©taill√©
            st.markdown(get_table_download_link(detailed_df, 'verbatims_par_theme.xlsx', 'T√©l√©charger le tableau d√©taill√© (Excel)'), unsafe_allow_html=True)

# Page principale de l'application
def main():
    st.markdown('<h1 class="main-header">üìä Analyse des Enqu√™tes de Satisfaction</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="highlight">
    Cette application permet d'analyser les enqu√™tes de satisfaction, avec un focus particulier sur l'identification
    des probl√©matiques r√©currentes dans les retours n√©gatifs. Utilis
