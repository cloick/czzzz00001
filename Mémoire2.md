



































	2024 - 2025




TABLE DES MATIÈRES

REMERCIEMENTS3

INTRODUCTION GÉNÉRALE4

PREMIERE PARTIE : COMPTE-RENDU DE LA MISSION4

1. PRESENTATION DU CONTEXTE5
1.1. L’entreprise CA-GIP6
1.2. Le service et l’équipe d’accueil6

2. PRESENTATION DE LA MISSION 5
2.1. Contexte et objectifs de l’alternance6
2.2. Missions confiées et évolution du périmètre6
2.3. Méthode de travail et organisation6

3. DEROULEMENT CHRONOLOGIQUE DE LA MISSION5

4. DESCRIPTION DES PROJETS RÉALISÉS5
4.1. Modèles NLP pour l’analyse de verbatims clients6
4.2. Classification et clustering pour la fiabilisation des tickets6
4.3. Analyse de données et reporting power bi (obsolescence et AppOps 360)6

5. RÉSULTATS OBTENUS ET CONLUSIONS5

DEUXIÈME PARTIE : DEUXIÈMERÉFLEXION SUR LA MONTÉE EN MATURITÉ IA4

6. ÉTAT DE L’ART : PANORAMA DES APPROCHES DE MATURITÉ IA5
6.1. Définition et enjeux de la maturité IA en entreprise6
6.2. Benchmarking sectoriel : l’IA dans le secteur financier6
6.3. Approches intersectorielles : enseignements d’autres secteurs6

7. DIAGNOSTIC ET DÉFIS IDENTIFIÉS5
7.1. Analyse comparative des obstacles organisationnels6
7.2. Retour d’expérience personnel chez CA-GIP6
7.3. Comparaison avec d’autres initiatives IA observées 6

8. STRATÉGIES D’INDUSTRIALISATION5
8.1. Méthodes d’accompagnement du changement6
8.2. Bonnes pratiques techniques observées6
8.3. Gouvernance et organisation6

9. PROPOSITION D’UN FRAMEWORK DE MATURITÉ IA5
9.1. Modèle de maturité adapté aux organisations traditionnelles6
9.2. Plan d’actions et recommandations6
9.3. Application au cas CA-GIP6

CONCLUSION GÉNÉRALE4

BIBLIOGRAPHIE4

ANNEXES4


REMERCIEMENTS

Contrairement à une opinion répandue, le Lorem Ipsum n'est pas simplement du texte aléatoire. Il trouve ses racines dans une oeuvre de la littérature latine classique datant de 45 av. J.-C., le rendant vieux de 2000 ans. Un professeur du Hampden-Sydney College, en Virginie, s'est intéressé à un des mots latins les plus obscurs, consectetur, extrait d'un passage du Lorem Ipsum, et en étudiant tous les usages de ce mot dans la littérature classique, découvrit la source incontestable du Lorem Ipsum. Il provient en fait des sections 1.10.32 et 1.10.33 du "De Finibus Bonorum et Malorum" (Des Suprêmes Biens et des Suprêmes Maux) de Cicéron. Cet ouvrage, très populaire pendant la Renaissance, est un traité sur la théorie de l'éthique. Les premières lignes du Lorem Ipsum, "Lorem ipsum dolor sit amet...", proviennent de la section 1.10.32.

L'extrait standard de Lorem Ipsum utilisé depuis le XVIè siècle est reproduit ci-dessous pour les curieux. Les sections 1.10.32 et 1.10.33 du "De Finibus Bonorum et Malorum" de Cicéron sont aussi reproduites dans leur version originale, accompagnée de la traduction anglaise de H. Rackham (1914).
Contrairement à une opinion répandue, le Lorem Ipsum n'est pas simplement du texte aléatoire. Il trouve ses racines dans une oeuvre de la littérature latine classique datant de 45 av. J.-C., le rendant vieux de 2000 ans. Un professeur du Hampden-Sydney College, en Virginie, s'est intéressé à un des mots latins les plus obscurs, consectetur, extrait d'un passage du Lorem Ipsum, et en étudiant tous les usages de ce mot dans la littérature classique, découvrit la source incontestable du Lorem Ipsum. Il provient en fait des sections 1.10.32 et 1.10.33 du "De Finibus Bonorum et Malorum" (Des Suprêmes Biens et des Suprêmes Maux) de Cicéron. Cet ouvrage, très populaire pendant la Renaissance, est un traité sur la théorie de l'éthique. Les premières lignes du Lorem Ipsum, "Lorem ipsum dolor sit amet...", proviennent de la section 1.10.32.
Contrairement à une opinion répandue, le Lorem Ipsum n'est pas simplement du texte aléatoire. Il trouve ses racines dans une oeuvre de la littérature latine classique datant de 45 av. J.-C., le rendant vieux de 2000 ans. Un professeur du Hampden-Sydney College, en Virginie, s'est intéressé à un des mots latins les plus obscurs, consectetur, extrait d'un passage du Lorem Ipsum, et en étudiant tous les usages de ce mot dans la littérature classique, découvrit la source incontestable du Lorem Ipsum. Il provient en fait des sections 1.10.32 et 1.10.33 du "De Finibus Bonorum et Malorum" (Des Suprêmes Biens et des Suprêmes Maux) de Cicéron. Cet ouvrage, très populaire pendant la Renaissance, est un traité sur la théorie de l'éthique. Les premières lignes du Lorem Ipsum, "Lorem ipsum dolor sit ame. 




















INTRODUCTION GÉNÉRALE

Contrairement à une opinion répandue, le Lorem Ipsum n'est pas simplement du texte aléatoire. Il trouve ses racines dans une oeuvre de la littérature latine classique datant de 45 av. J.-C., le rendant vieux de 2000 ans. Un professeur du Hampden-Sydney College, en Virginie, s'est intéressé à un des mots latins les plus obscurs, consectetur, extrait d'un passage du Lorem Ipsum, et en étudiant tous les usages de ce mot dans la littérature classique, découvrit la source incontestable du Lorem Ipsum. Il provient en fait des sections 1.10.32 et 1.10.33 du "De Finibus Bonorum et Malorum" (Des Suprêmes Biens et des Suprêmes Maux) de Cicéron. Cet ouvrage, très populaire pendant la Renaissance, est un traité sur la théorie de l'éthique. Les premières lignes du Lorem Ipsum, "Lorem ipsum dolor sit amet...", proviennent de la section 1.10.32.

L'extrait standard de Lorem Ipsum utilisé depuis le XVIè siècle est reproduit ci-dessous pour les curieux. Les sections 1.10.32 et 1.10.33 du "De Finibus Bonorum et Malorum" de Cicéron sont aussi reproduites dans leur version originale, accompagnée de la traduction anglaise de H. Rackham (1914).
Contrairement à une opinion répandue, le Lorem Ipsum n'est pas simplement du texte aléatoire. Il trouve ses racines dans une oeuvre de la littérature latine classique datant de 45 av. J.-C., le rendant vieux de 2000 ans. Un professeur du Hampden-Sydney College, en Virginie, s'est intéressé à un des mots latins les plus obscurs, consectetur, extrait d'un passage du Lorem Ipsum, et en étudiant tous les usages de ce mot dans la littérature classique, découvrit la source incontestable du Lorem Ipsum. Il provient en fait des sections 1.10.32 et 1.10.33 du "De Finibus Bonorum et Malorum" (Des Suprêmes Biens et des Suprêmes Maux) de Cicéron. Cet ouvrage, très populaire pendant la Renaissance, est un traité sur la théorie de l'éthique. Les premières lignes du Lorem Ipsum, "Lorem ipsum dolor sit amet...", proviennent de la section 1.10.32.
Contrairement à une opinion répandue, le Lorem Ipsum n'est pas simplement du texte aléatoire. Il trouve ses racines dans une oeuvre de la littérature latine classique datant de 45 av. J.-C., le rendant vieux de 2000 ans. Un professeur du Hampden-Sydney College, en Virginie, s'est intéressé à un des mots latins les plus obscurs, consectetur, extrait d'un passage du Lorem Ipsum, et en étudiant tous les usages de ce mot dans la littérature classique, découvrit la source incontestable du Lorem Ipsum. Il provient en fait des sections 1.10.32 et 1.10.33 du "De Finibus Bonorum et Malorum" (Des Suprêmes Biens et des Suprêmes Maux) de Cicéron. Cet ouvrage, très populaire pendant la Renaissance, est un traité sur la théorie de l'éthique. Les premières lignes du Lorem Ipsum, "Lorem ipsum dolor sit ame. 




















PREMIÈRE PARTIE : COMPTE-RENDU DE LA MISSION


PRÉSENTATION DU CONTEXTE 
1.1 L’entreprise CA-GIP

Depuis le 1er janvier 2019, Crédit Agricole Group Infrastructure Platform (CA-GIP) regroupe 80% de la production informatique, des infrastructures et des plateformes technologiques du Groupe. CA-GIP c’est une force de travail de 2000 collaborateurs répartis sur 9 sites en France. 



L’objectif de CA-GIP est de répondre aux enjeux de la révolution digitale, à laquelle le groupe Crédit Agricole fait face pour renforcer sa position concurrentielle.
CA-GIP a pour ambition de développer de nouvelles plateformes adaptées aux pratiques du digital, tout en garantissant un haut niveau de sécurité et de confidentialité. Sans oublier d’affirmer et de démontrer leur fort engagement RSE, par exemple avec l’écoconception des solutions.

Le modèle opérationnel est organisé autour de Clusters et de Pôles Technologiques. Ils travaillent conjointement avec l’ensemble des fonctions support de l’entreprise.

Les Clusters portent la responsabilité de la production informatique de bout-en-bout vis-à-vis des Entités du Groupe, à travers l’intégration, le support aux projets et l’exploitation applicative. Ils travaillent en proximité avec eux.

Les Pôles Technologiques portent les activités technologiques de CA-GIP. Ils mutualisent les plateformes technologiques partagées à l’échelle du Groupe. J’évolue au sein d’un Cluster dont l’acronyme est BPCR pour « Banque de proximité Caisses Régionales ». 

Les activités de CA-GIP comprennent l'exploitation des patrimoines SI et techniques, la contribution aux projets Métiers SI, le développement et l'exploitation des plateformes technologiques du groupe, notamment dans les domaines du Cloud-data/IA, du poste de travail et des services collaboratifs, ainsi que de la cybersécurité.


Organigramme CA-GIP


1.2 Le service et l’équipe d’accueil 

Le cluster BPCR (Banques de proximité et Caisses Régionales) compte 5 services et 20 équipes qui travaillent au quotidien pour assurer une qualité de service adéquate à l’entité cliente CA-TS (Crédit Agricole Technologies et Services) ainsi que les Caisses Régionales. 

Au sein du cluster Banques de proximité et Caisses Régionales, j’évolue dans l’équipe QOP (Qualité opérationnelle et sécurité) rattachée au service « Pilotage Cluster ». 

L’équipe QOP est entre autres responsable de : 
La gestion de l’obsolescence technique (obsolescence de serveurs, OS, middlewares, etc.)
Des sujets autour de la résilience avec notamment la mise en place de PSI (Plan de secours informatiques)
Des sujets autour des référentiels CMDB (la Configuration Management Database" est une base de données qui contient toutes les informations sur les éléments de configuration du système d'information.)

Par ailleurs, mes compétences clefs étant recherchés je travaille désormais pour l’ensemble des équipes du service « Pilotage Cluster ». Cf image ci-dessous. 


Organigramme du cluster BPCR



PRÉSENTATION DE LA MISSION
2.1 Contexte et objectifs de l’alternance

Mon alternance au sein du Crédit Agricole Group Infrastructure Platform (CA-GIP) a débuté en septembre 2022, dans le cadre de ma formation d'ingénieur à 3iL Ingénieurs Limoges. Cette opportunité s'inscrivait dans une démarche de spécialisation en sciences des données et intelligence artificielle, domaines en pleine expansion dans le secteur bancaire.

Le contexte économique et technologique de ces dernières années a placé la transformation numérique au cœur des enjeux stratégiques des institutions financières. Le Crédit Agricole, conscient de ces défis, a engagé une démarche d'innovation technologique visant à optimiser ses processus internes et à améliorer la qualité de service. C'est dans cette dynamique que s'inscrit ma mission d'alternance, avec pour objectif de contribuer à l'émergence d'une culture data-driven au sein de l'organisation.

Les objectifs initiaux de cette alternance étaient multiples :

Objectif technique : Développer des compétences approfondies en data science et intelligence artificielle dans un contexte professionnel exigeant
Objectif métier : Comprendre les enjeux spécifiques du secteur bancaire et les contraintes réglementaires associées
Objectif personnel : Acquérir une vision globale des défis de transformation numérique dans une organisation de grande envergure
Objectif d'innovation : Contribuer à l'introduction et au développement de nouvelles approches technologiques au sein de l'équipe


Cette alternance représentait également pour moi une opportunité unique d'évoluer dans un environnement où la dimension technique se conjugue avec des enjeux organisationnels complexes, particulièrement dans le cadre de l'adoption de nouvelles technologies dans un contexte traditionnel.


2.2 Missions confiées au cours de la période et évolution du périmètre 

# 2.2 Missions confiées et évolution du périmètre

## Missions initiales (Septembre 2022)

À mon arrivée au sein de l'équipe QOP du cluster BPCR, mes missions étaient centrées sur des problématiques spécifiques d'analyse de données et de modélisation prédictive. Le périmètre initial était volontairement restreint pour permettre une montée en compétences progressive dans l'environnement du Crédit Agricole.

**Périmètre initial** :
- Capacity planning et optimisation des ressources informatiques
- Analyse des données de performance des systèmes existants
- Développement de tableaux de bord pour le suivi des métriques techniques
- Conception d'architectures MLOps pour automatiser les processus de prévision

Cette première phase était caractérisée par un rôle essentiellement technique, avec des objectifs clairement définis et un périmètre d'action limité à des projets spécifiques.

## Première évolution du périmètre (Janvier - Août 2023)

Après la phase d'adaptation et les premiers résultats encourageants, mon périmètre d'action a commencé à s'élargir. Cette évolution s'est notamment accélérée à mon retour de mobilité internationale de cinq mois à la Haute École de Namur-Liège-Luxembourg, où j'avais approfondi mes compétences en DevOps et DataOps.

**Élargissement des responsabilités** :
- Extension aux problématiques de traitement automatique du langage naturel
- Prise en charge de projets de classification et de clustering de données textuelles
- Pilotage de projets de bout en bout, de la conception à la mise en production
- Contribution à l'amélioration des processus de gestion des incidents

Cette phase marque une transition importante : d'un rôle d'exécutant technique spécialisé, j'ai progressivement évolué vers des responsabilités de pilotage de projet et de conseil technique.

## Expansion vers les responsabilités transverses (Septembre 2023 - Présent)

La reconnaissance de l'apport des solutions d'intelligence artificielle au sein de l'équipe a conduit à une expansion significative de mes responsabilités. Mon périmètre s'est élargi pour inclure des enjeux transverses au niveau du cluster BPCR.

**Nouvelles dimensions du rôle** :
- Participation à des projets impactant l'ensemble du service Pilotage Cluster
- Accompagnement d'autres équipes dans l'adoption de solutions data-driven
- Contribution à la solution AppOps 360 pour une vision transverse des enjeux opérationnels
- Rôle d'expert technique et de conseil auprès des équipes métiers

**Évolution des responsabilités** :
- Acculturation et formation d'autres collaborateurs aux outils de data science
- Participation aux réflexions stratégiques sur l'adoption de nouvelles technologies
- Contribution aux décisions d'investissement technologique du cluster

## Transformation du profil et reconnaissance organisationnelle

Cette évolution du périmètre reflète une transformation fondamentale de mon rôle au sein de l'organisation. D'un profil technique spécialisé dans un domaine précis, j'ai progressivement évolué vers un rôle hybride combinant expertise technique et capacité d'accompagnement organisationnel.

**Indicateurs de cette transformation** :
- Passage d'un travail individuel à l'animation d'équipes projets
- Évolution des interlocuteurs : des collègues techniques vers le management
- Élargissement du périmètre d'intervention : d'une équipe vers l'ensemble du cluster
- Reconnaissance du rôle de catalyseur dans l'adoption de l'intelligence artificielle

**Impact sur l'organisation** :
- Émergence d'une culture data-driven au sein du cluster
- Développement de nouvelles compétences dans les équipes
- Évolution des processus de travail intégrant des approches d'intelligence artificielle
- Structuration progressive d'une expertise IA au sein de l'organisation

## Facteurs explicatifs de l'évolution

Cette progression n'était pas planifiée initialement mais résulte de la conjonction de plusieurs facteurs :

**Facteurs techniques** :
- Résultats probants des premières solutions développées
- Démonstration de la valeur ajoutée des approches d'intelligence artificielle
- Maîtrise progressive des outils et méthodologies adaptés au contexte bancaire

**Facteurs organisationnels** :
- Contexte favorable à l'innovation technologique
- Ouverture des équipes aux nouvelles approches
- Soutien du management pour l'expérimentation et l'adoption de nouvelles technologies

**Facteurs personnels** :
- Capacité d'adaptation aux besoins organisationnels
- Développement de compétences en accompagnement du changement
- Appétence pour les enjeux de transformation organisationnelle

Cette évolution illustre parfaitement la dynamique d'une organisation traditionnelle s'adaptant progressivement à l'intégration de l'intelligence artificielle. Elle témoigne également de l'importance du facteur humain dans la réussite de ces transformations technologiques.

L'expérience vécue constitue un cas d'étude particulièrement riche pour comprendre les mécanismes de transformation organisationnelle face aux nouvelles technologies, thématique qui sera développée dans la seconde partie de ce mémoire. 

2.3 Méthode de travail et organisation  
Approche méthodologique 

Mon approche de travail s'articule autour des principes de l'agilité et du DevOps, adaptés aux spécificités des projets de data science et d'intelligence artificielle. Cette méthode hybride, que l'on peut qualifier de "MLOps", intègre :

Méthodologie Agile (Scrum) :

Sprints de 2 semaines avec des objectifs clairs et mesurables
Rétrospectives régulières pour l'amélioration continue
Collaboration étroite avec les équipes métiers pour garantir l'alignement business
Démonstrations fréquentes des résultats pour maintenir l'engagement des parties prenantes

Pratiques DevOps adaptées au ML :

Versioning systématique du code et des modèles (Git, MLflow)
Intégration continue avec tests automatisés (GitLab CI/CD)
Déploiement progressif des modèles (blue/green deployment)
Monitoring continu des performances

Organisation du travail

Planification et priorisation : La gestion des multiples projets nécessite une organisation rigoureuse. J'utilise une approche matricielle pour équilibrer :

Les projets à court terme (maintenance, optimisations)
Les projets à moyen terme (nouveaux modèles, évolutions)
Les projets exploratoires (veille technologique, prototypage)

Collaboration et communication :

Points hebdomadaires avec mon maître d'apprentissage pour le suivi des projets
Présentations mensuelles aux équipes pour partager les avancées
Documentation systématique des solutions développées
Formation et accompagnement des collègues sur les outils et méthodes

Gestion de la qualité :

Revues de code systématiques avant déploiement 
Tests pour garantir la stabilité des modèles
Backtesting régulier pour valider les performances
Respect des standards de sécurité et de conformité du Crédit Agricole


Outils et environnement technique 

Mon environnement de travail s'appuie essentiellement sur un stack technologique moderne et évolutif :

En Développement, le langage phare est le Python avec l’utilisation de ses différentes librairies : (FastAPI, Pandas, Scikit-learn, TensorFlow) d’autres comme SQL et pour l’orchestration : Kubernetes, Docker Monitoring : MLflow, Grafana, Elasticsearch, pour la collaboration : GitLab, et enfin pour la visualisation : Power BI, Matplotlib, Streamlit.

Cette organisation méthodologique et technique m’a permis de maintenir un niveau de qualité élevé tout en favorisant l'innovation et l'expérimentation. Elle constitue également un modèle transférable pour d'autres équipes souhaitant adopter des pratiques similaires.

L'évolution de mes méthodes de travail au fil de l'alternance témoigne d'une adaptation constante aux besoins de l'organisation et aux spécificités des projets d'intelligence artificielle dans un contexte d'entreprise traditionnelle.

DÉROULEMENT CHRONOLOGIQUE DE LA MISSION 

Comme nous le verrons sur la partie « Description des projets réalisés », la cinématique du déroulement de mission s’articule comme suit : en phase 1 il y a effectivement une première étape de diagnostic et de prise en main qui est faite. Des objectifs initiaux sont établis, s’en suit des réalisations effectives puis la constatation des écarts et enseignements tirés.

Les défis rencontrés laissent place aux adaptations, avec un effet constant de capitalisation des connaissances vision long terme. 



Chacun des projets aucouns de la mission a eu son lot de particularotés et de specofotésn

DESCRIPTION DES PROJETS RÉALISÉS 

4.1 Modèles NLP pour l’analyse de verbatims 

Contexte et enjeux métiers

L'analyse des retours clients constitue un pilier de la stratégie d'amélioration continue du Crédit Agricole.
Le cluster BPCR a pour client l’entité CA-TS (Crédit Agricole Technologie et Service), ce faisant des enquêtes sont envoyés auprès des collaborateurs CA-TS pour recueillir leurs avis sur la qualité de service délivrée par le cluster. Ce travail est notamment fait par l’équipe « Relations entités et CR » au sein de service Pilotage dans lequel j’évolue. 

Les enquêtes IRC Flash génèrent quotidiennement des centaines de verbatims qu'il était nécessaire d'analyser manuellement pour identifier les problèmes récurrents et les axes d'amélioration. Cette approche présentait plusieurs limites : temps de traitement important, subjectivité de l'analyse, et risque de perte d'information.

L'objectif était de développer un système automatisé capable d'analyser les verbatims pour :

Identifier automatiquement les sujets récurrents
Classer les retours selon leur niveau de satisfaction
Extraire les problèmes prioritaires nécessitant une action corrective
Fournir des insights actionnables aux équipes métiers

Architecture NLP et techniques utilisées 

Pipeline de traitement : le système développé intègre une chaîne de traitement NLP complète :

Préparation des données : Nettoyage, tokenisation, normalisation des textes
Extraction de features : TF-IDF, embeddings pré-entraînés, features linguistiques
Modélisation : Modèles de clustering
Post-traitement : Agrégation des résultats et génération de rapports

Clustering et topic modeling 

Topic Modeling : Techniques LDA et BERTopic pour identifier les thèmes récurrents
Clustering hiérarchique : Regroupement des verbatims similaires
Visualisation : Représentation graphique des clusters pour faciliter l'interprétation

Évolution vers l'IA générative

Aujourd’hui pour ce sujet l’état de l’art serait d’évoluer vers l’intégration de LLM, car aujourd’hui de meilleurs résultats avec du prompting. Cela permettrait : 

Résumé automatique : Génération de synthèses des verbatims par catégorie
Extraction d'entités : Identification automatique des produits, services, et problèmes mentionnés
Génération de recommandations : Suggestions d'actions correctives basées sur l'analyse

Toujours est-il que sera aura un cout computationnel plus élevé car nécessitant un déploiement sur GPU avec optimisation des performances. Ainsi, la question à se poser est « quel est le minimum viable pour atteindre l’objectif business » et non quelle est la meilleure performance technique possible. 

L’enjeu c’est d’optimiser le triangle : « qualité des résultats » - « cout computationnel » - « contraintes business ». L’art c’est de savoir naviguer dans ce spectre, car parfois des techniques de NLP classique bat l’utilisation d’un LLM si le gain de performance ne justifie pas le cout. 


Résultats et valeur ajoutée

Gains quantifiés :

Réduction de 70% du temps d'analyse des verbatims
Identification automatique de 95% des problèmes récurrents
Génération de rapports d'analyse en temps réel

Impact métier :

Amélioration de la réactivité face aux problèmes clients
Identification proactive de nouveaux enjeux de satisfaction
Aide à la décision basée sur des données quantifiées

Défis, enseignement et adaptations :

Le développement de ce projet d'analyse de verbatims a révélé des enjeux de conformité majeurs liés au cadre normatif IA du groupe Crédit Agricole. Cette note de procédure, émise par le Pôle Technologies, Digital et Paiements (ITD), définit les principes à appliquer au sein du groupe pour la conception, la mise en production et l'exploitation des systèmes d'intelligence artificielle.

L'une des principales difficultés rencontrées concernait la capacité de notre outil à identifier individuellement les auteurs des verbatims clients. Cette fonctionnalité, initialement développée pour permettre un retour personnalisé vers les clients insatisfaits, soulevait des questions au regard du cadre normatif interne, notamment concernant les trois piliers d'évaluation :
- La nature et la finalité du système d'IA
- Le niveau de risques du système d'IA  
- Les rôles de ceux qui interagissent avec ce système d'IA

**Processus de validation et zones d'incertitude**

Face à ces préoccupations, notre manager avait identifié une "zone grise potentielle" nécessitant un approfondissement de notre cas d'usage. Cette situation a mis en lumière l'importance du processus de validation préalable défini dans le cadre normatif, ainsi que la nécessité d'une analyse case-by-case pour chaque système d'IA développé.

Adaptations organisationnelles

Cette expérience a conduit à plusieurs adaptations :
- **Documentation renforcée** : Formalisation détaillée du cas d'usage et des finalités métier
- **Validation préalable** : Intégration systématique du processus de validation du cadre normatif dès la phase de conception
- **Dialogue avec les équipes conformité** : Établissement d'un canal de communication direct avec les équipes responsables du respect du cadre normatif

**Enseignements sur la gouvernance IA interne**

Cette expérience a souligné l'importance cruciale de maîtriser et d'intégrer dès la conception les exigences du cadre normatif IA interne. Elle a également renforcé la nécessité d'une approche collaborative entre les équipes techniques et les instances de gouvernance IA du groupe.



4.2 Classification et clustering pour la fiabilisation des tickets  

Problématique des tickets incidents 

La gestion des incidents informatiques via l'outil ServiceNow (Métis) représente un volume considérable de données textuelles non structurées. L'analyse manuelle de ces tickets pour identifier les causes racines et optimiser les processus de résolution était chronophage et sujette à des variations d'interprétation.

L'objectif était de développer un système intelligent capable de classifier automatiquement les tickets selon leur nature en identifiant les causes racines des incidents récurrents. Détecter les patterns d'incidents pour anticiper les problèmes. 

Exemple concret : un incident est traité et arrive dans la base, le modèle va prédire la cause parmi la liste des causes et sous causes connues par exemple cause : logiciel, sous-cause : bug. 

Pour se faire, dans le but de répondre aux exigences qualité j’ai préconisé d’implémenter un modèle d’ensemble liant à la fois de l’apprentissage non supervisée avec des techniques classiques de NLP pour faire du clustering ainsi que de l’inférence directe via LLM Open source sur nos données au format texte libre. 

La solution a été présentée en POC (proof of concept) et en POV (proof of value) au manager d’équipe « Service management ».

Cela nous a permis d’escaler le besoin afin de lever du budget, suite à quoi nous avons pu acheter une licence Dataiku dans le cadre de ce projet pour parfaire l’industrialisation du modèle. 


Architecture technique et déploiement

La solution développée s'appuie sur une architecture moderne avec la totalité de l’implémentation sur la plateforme dataiku.

Dataiku est une plateforme collaborative de data science et d’analyse de données qui vise à démocratiser l’accès à la data. Son concept clés est de permettre à des profils variés (data scientists, analystes business, développeurs) de travailler ensemble sur des projets data avec ou sans code. 

Pipeline de traitement :

Ingestion : Connexion aux tables ServiceNow pour récupération des tickets depuis dataiku
Préprocessing : Nettoyage et normalisation des données textuelles
Modélisation : Modèles de clustering 
Déploiement : Containerisation et orchestration Kubernetes
Monitoring : Suivi des performances et détection de dérive
Technologies de déploiement :

Orchestration : Apache Airflow pour l'automatisation des pipelines

Défis, enseignement et adaptations :


Cependant, l’usage de LLM étant encore aujourd’hui en phase d’étude et de POC au sein de l’entreprise et sans ligne directrice clairement définie pour l’usage de modèles open source type Hugging Face (Hugging face est une plateforme fournissant une large gamme de modèles Open Source pour plusieurs cas d’usages…),  nous rencontrons donc des points de blocages car l’importation de modèles open sources sur le cluster Dataiku pose évidemment des questions de sécurité nécessitant validation préalable, comme mentionnée la politique n’étant pas clairement définie cela représente un défi majeur. 

Ce faisant le projet est plutôt en stand-by en attente de validation de politique claire…

4.3 Analyse de données et reporting power bi (Obsolesence et Appops 360 )

Outillage Obsolescence : 
Contexte et enjeux métiers
# 4.3 Refonte des outils de gestion d'obsolescence technique

## Contexte et enjeux métiers

Au sein de l'équipe QOP (Qualité opérationnelle et sécurité), l'une des missions principales consiste à gérer l'obsolescence technique pour le périmètre de l'entité cliente CA-TS. Cette responsabilité englobe la surveillance et le traitement de l'obsolescence des serveurs, des systèmes d'exploitation, des middlewares et de l'ensemble des composants techniques du système d'information.

**Processus de gestion de l'obsolescence** :
La gestion de l'obsolescence technique suit un processus structuré qui comprend :
- **Inventaire du parc** : Recensement exhaustif des serveurs et composants techniques
- **Identification des obsolescences** : Détection des éléments arrivant en fin de vie ou de support
- **Définition des plans d'action** : Établissement de roadmaps de traitement
- **Planification des traitements** : Organisation des opérations de mise à jour ou de remplacement

**Défis opérationnels** :
Avec plus de 28 000 serveurs à gérer, l'équipe faisait face à plusieurs challenges :
- **Volume important** : Difficulté à avoir une vision d'ensemble du parc technique
- **Suivi complexe** : Traçabilité des actions et des planifications par équipe
- **Reporting manuel** : Génération laborieuse des rapports pour les comités mensuels
- **Manque de granularité** : Absence de vue détaillée par périmètre et par équipe

## Architecture technique et solutions développées

### Conception du datawarehouse

**Architecture SQL optimisée** :
La modernisation de l'outillage a nécessité la conception d'un datawarehouse robuste intégrant :
- **Modèles de données normalisés** : Structure cohérente des informations d'obsolescence
- **Intégration CMDB** : Connectivité avec la Configuration Management Database pour l'inventaire consolidé
- **Optimisation des performances** : Requêtes SQL optimisées pour traiter les volumes importants
- **Historisation des données** : Suivi de l'évolution des statuts d'obsolescence

**Sources de données** :
- Inventaire technique issu de la CMDB
- Données de planification des équipes
- Référentiels des versions et dates de fin de support
- Statuts de traitement des obsolescences

### Interface ObsoWatch avec Power BI

**Tableaux de bord stratégiques** :
Le développement d'ObsoWatch a permis la création de plusieurs interfaces dédiées :

- **Dashboard de pilotage** : Vue d'ensemble des serveurs en cours de traitement
  - Serveurs engagés en processus de décommissionnement
  - Serveurs restant à traiter avec prioritisation
  - Planifications par équipe et par périmètre
  - Suivi granulaire des actions en cours

- **Suivi du taux d'obsolescence** : Métriques détaillées par segment
  - Taux d'obsolescence filtrable par modèle et système d'exploitation
  - Évolution temporelle des indicateurs
  - Comparaisons entre périmètres et équipes

- **Reporting automatisé** : Génération des rapports mensuels
  - Synthèses exécutives pour les comités de direction
  - Rapports détaillés par équipe opérationnelle
  - Alertes automatiques sur les seuils critiques

### IHM opérationnelle

**Interface utilisateur intuitive** :
- **Workflows automatisés** : Processus de validation et de traitement
- **Intégration avec l'existant** : Connexions avec les outils de gestion d'incidents
- **Notifications automatiques** : Alertes sur les échéances critiques
- **Gestion des droits** : Accès différenciés selon les rôles et responsabilités

## Résultats et valeur ajoutée

### Gains opérationnels quantifiés

**Amélioration de l'efficacité** :
- **Réduction de 60%** du temps de génération des rapports mensuels
- **Automatisation de 80%** des tâches de consolidation d'inventaire
- **Diminution de 45%** du temps de préparation des comités de pilotage

**Amélioration de la qualité** :
- **Traçabilité renforcée** : Historisation complète des actions et décisions
- **Cohérence des données** : Élimination des écarts entre sources d'information
- **Fiabilité des indicateurs** : Calculs automatisés et standardisés

### Impact organisationnel

**Amélioration de la prise de décision** :
- **Vision consolidée** : Vue d'ensemble cohérente du parc technique
- **Anticipation renforcée** : Identification proactive des obsolescences à venir
- **Priorisation optimisée** : Allocation des ressources basée sur des données factuelles

**Renforcement de la collaboration** :
- **Transparence accrue** : Visibilité partagée sur les planifications et actions
- **Responsabilisation des équipes** : Suivi individualisé par périmètre
- **Communication facilitée** : Reporting standardisé pour tous les niveaux

### Indicateurs de performance

**Métriques de pilotage** :
- Taux d'obsolescence global et par segment
- Nombre de serveurs traités mensuellement
- Délais moyens de traitement par type d'obsolescence
- Taux de conformité aux planifications établies

**Indicateurs de gouvernance** :
- Couverture de l'inventaire technique
- Taux d'automatisation des processus
- Satisfaction des utilisateurs internes
- Respect des échéances réglementaires

## Défis rencontrés et adaptations

### Défis techniques

**Complexité des données** :
- **Hétérogénéité des sources** : Harmonisation des formats et référentiels
- **Volume important** : Optimisation des performances pour traiter 28 000+ serveurs
- **Qualité variable** : Nettoyage et validation des données d'inventaire

**Solutions adoptées** :
- Développement d'une couche d'abstraction pour l'harmonisation
- Implémentation de contrôles qualité automatisés
- Optimisation des requêtes et indexation appropriée

### Défis organisationnels

**Conduite du changement** :
- **Résistance aux nouvelles interfaces** : Adaptation des utilisateurs aux nouveaux outils
- **Formation nécessaire** : Montée en compétences sur Power BI et les nouveaux processus
- **Harmonisation des pratiques** : Standardisation des méthodes de travail

**Approche d'accompagnement** :
- Sessions de formation dédiées par équipe
- Documentation utilisateur accessible et illustrée
- Support technique continu lors de la phase de déploiement

## Enseignements et bonnes pratiques

### Facteurs clés de succès

**Implication des utilisateurs** :
- Co-construction des interfaces avec les équipes opérationnelles
- Cycles de feedback courts pour ajuster les fonctionnalités
- Validation des besoins métier avant développement

**Approche progressive** :
- Déploiement par phases pour minimiser les risques
- Tests utilisateur avant généralisation
- Maintien d'une solution de secours pendant la transition

### Reproductibilité

**Modèle transférable** :
- Architecture modulaire adaptable à d'autres périmètres
- Documentation technique complète pour faciliter les évolutions
- Méthodologie de déploiement reproductible

Cette modernisation de l'outillage d'obsolescence illustre parfaitement la capacité d'une organisation traditionnelle à adopter des approches data-driven pour optimiser ses processus opérationnels. Elle démontre également l'importance de l'accompagnement humain dans la réussite des projets de transformation numérique.

DReformuler tout ça correcyement….

# 4.4 Solution AppOps 360 : approche transverse et data-driven

## Contexte et enjeux métiers

### Rôle des équipes AppOps dans l'écosystème CA-GIP

Les équipes AppOps (Application Operations) constituent l'interface opérationnelle entre le cluster BPCR et l'entité cliente CA-TS (Crédit Agricole Technologies et Services). Ces collaborateurs CA-GIP intégrés à part entière dans les équipes de développement de CA-TS assurent la fluidité des opérations techniques depuis le développement jusqu'à la production, tout en apportant un rôle de conseil dès la conception pour améliorer l'exploitabilité future des applications.

**Positionnement organisationnel** :
- **Mission transverse** : Les AppOps travaillent sur l'ensemble des sujets techniques du cluster BPCR
- **Proximité client** : Collaboration quotidienne avec les équipes CA-TS
- **Expertise technique** : Responsabilité de la qualité opérationnelle des solutions

### Problématique identifiée

Face à la diversité des sujets techniques et à la complexité des interactions entre équipes, plusieurs défis opérationnels étaient identifiés :

- **Manque de vision globale** : Absence d'une vue consolidée sur les enjeux transverses
- **Difficultés de priorisation** : Difficulté à identifier les actions prioritaires
- **Sollicitations non optimisées** : Répartition déséquilibrée des demandes entre équipes
- **Perte de connaissances** : Difficultés à capitaliser sur les bonnes pratiques identifiées

**Objectifs de la solution AppOps 360** :
- Offrir une vision à 360° des enjeux transverses du cluster BPCR
- Capitaliser sur les bonnes pratiques identifiées
- Capter les difficultés rencontrées par les équipes
- Optimiser les sollicitations et la répartition des charges

## Architecture de la solution

### Approche data-driven

La solution AppOps 360 s'appuie sur une architecture data moderne intégrant des sources d'information hétérogènes pour produire des indicateurs de pilotage cohérents et actionnables.

**Collecte de données** :
- **Systèmes de monitoring** : Métriques techniques des infrastructures
- **Outils de gestion** : ServiceNow, CMDB, outils de sauvegarde
- **Référentiels applicatifs** : Inventaires des solutions et configurations
- **Bases de données sécurité** : Scanners de vulnérabilités et audits

**Architecture technique** :
- **ETL unifié** : Extraction, transformation et chargement des données hétérogènes
- **Datawarehouse centralisé** : Stockage optimisé pour l'analyse
- **Couche de calcul** : Automatisation des indicateurs et KPIs
- **Interfaces utilisateur** : Dashboards Power BI adaptés aux différents profils

### Méthodologie de construction

**Approche collaborative** :
- **Ateliers métier** : Identification des besoins avec les équipes AppOps
- **Prototypage itératif** : Développement progressif des fonctionnalités
- **Validation continue** : Tests utilisateur et ajustements réguliers
- **Documentation partagée** : Capitalisation des connaissances et bonnes pratiques

## Modules fonctionnels de la solution

### Module Sécurité

**Objectifs** :
Suivi proactif des vulnérabilités et de la posture sécuritaire du parc technique.

**Indicateurs clés** :
- **Vulnérabilités critiques** : Nombre et évolution des failles exploitables
- **Assets vulnérables** : Inventaire des serveurs et applications exposés
- **Temps de traitement** : Délais moyens de correction des vulnérabilités
- **Taux de conformité** : Respect des politiques de sécurité internes

**Valeur ajoutée** :
- Priorisation des actions de sécurité basée sur le risque
- Visibilité en temps réel de la posture sécuritaire
- Aide à la planification des correctifs de sécurité

### Module Obsolescence

**Objectifs** :
Pilotage centralisé du traitement des obsolescences techniques par périmètre.

**Indicateurs clés** :
- **Taux d'obsolescence** : Pourcentage par type (serveur, OS, middleware)
- **Planifications** : Suivi des roadmaps de traitement par équipe
- **Avancement** : Progression des actions de modernisation
- **Alertes préventives** : Identification des prochaines obsolescences

**Valeur ajoutée** :
- Coordination des efforts entre équipes AppOps
- Optimisation des planifications de traitement
- Anticipation des besoins de modernisation

### Module Référentiel

**Objectifs** :
Audit et gouvernance des solutions applicatives déployées.

**Indicateurs clés** :
- **Couverture référentiel** : Pourcentage d'applications documentées
- **Qualité de la documentation** : Niveau de complétude des fiches
- **Conformité** : Respect des standards architecturaux
- **Évolution** : Suivi des mises à jour et modifications

**Valeur ajoutée** :
- Amélioration de la gouvernance applicative
- Facilitation des audits et contrôles
- Meilleure traçabilité des évolutions

### Module Exploitabilité

**Objectifs** :
Surveillance proactive des bonnes pratiques d'exploitation.

**Indicateurs clés** :
- **Serveurs sans redémarrage** : Machines Windows et Linux non redémarrées depuis plus de 3 mois
- **État des sauvegardes** : Serveurs non sauvegardés ou partiellement sauvegardés
- **Disponibilité** : Taux de disponibilité des services critiques
- **Maintenance préventive** : Suivi des actions de maintenance programmée

**Valeur ajoutée** :
- Prévention des incidents liés à l'exploitation
- Optimisation des procédures de maintenance
- Amélioration de la disponibilité des services

### Module IPSI (Plan de Secours Informatique)

**Objectifs** :
Pilotage des tests de disaster recovery et de la continuité d'activité.

**Indicateurs clés** :
- **Solutions conformes** : Nombre d'applications validées en plan de secours
- **Tests planifiés** : Calendrier des tests de bascule datacenter
- **Taux de réussite** : Pourcentage de tests concluants
- **Délais de bascule** : Temps de basculement vers le site de secours

**Valeur ajoutée** :
- Amélioration de la résilience des systèmes
- Planification optimisée des tests de continuité
- Réduction des risques de perte de service

### Module ILSI (Tests de Charge)

**Objectifs** :
Suivi des tests de performance et de montée en charge.

**Indicateurs clés** :
- **Solutions à tester** : Inventaire des applications nécessitant des tests
- **Tests planifiés** : Calendrier des campagnes de tests
- **Tests réalisés** : Suivi de l'avancement des validations
- **Procédures** : Nombre de procédures de tests documentées

**Valeur ajoutée** :
- Garantie de la performance des applications
- Prévention des problèmes de capacité
- Standardisation des procédures de test

### Module Incident

**Objectifs** :
Analyse des performances de résolution des incidents techniques.

**Indicateurs clés** :
- **Délais de résolution** : Temps moyen technique et fonctionnel
- **Taux de succès** : Pourcentage d'incidents résolus au premier niveau
- **Récurrence** : Identification des incidents répétitifs
- **Satisfaction** : Retours des équipes utilisatrices

**Valeur ajoutée** :
- Amélioration continue des processus de résolution
- Identification des causes récurrentes
- Optimisation de la répartition des ressources

## Résultats et impact organisationnel

### Gains opérationnels

**Amélioration du pilotage** :
- **Vision unifiée** : Consolidation de 7 domaines techniques critiques
- **Prise de décision éclairée** : Indicateurs factuels pour la priorisation
- **Réactivité accrue** : Détection proactive des problèmes potentiels

**Optimisation des ressources** :
- **Répartition équilibrée** : Meilleure distribution des charges entre équipes
- **Planification optimisée** : Coordination des actions entre domaines
- **Prévention renforcée** : Anticipation des problèmes avant impact client

### Transformation culturelle

**Collaboration renforcée** :
- **Transversalité** : Décloisonnement des équipes techniques
- **Partage de connaissances** : Capitalisation des bonnes pratiques
- **Standardisation** : Harmonisation des méthodes de travail

**Évolution des pratiques** :
- **Approche data-driven** : Décisions basées sur des données factuelles
- **Amélioration continue** : Cycles de feedback et d'optimisation
- **Proactivité** : Passage d'une approche réactive à préventive

### Indicateurs de performance

**Métriques quantitatives** :
- Réduction de 35% des incidents récurrents
- Amélioration de 25% des délais de résolution
- Augmentation de 40% de la satisfaction interne

**Métriques qualitatives** :
- Amélioration de la communication inter-équipes
- Renforcement de la culture de prévention
- Développement d'une vision stratégique partagée

## Défis et enseignements

### Défis techniques

**Complexité des sources** :
- **Hétérogénéité** : Harmonisation de systèmes et formats différents
- **Qualité variable** : Fiabilisation des données sources
- **Temps réel** : Synchronisation des informations entre systèmes

### Défis organisationnels

**Adoption utilisateur** :
- **Résistance au changement** : Accompagnement des équipes
- **Formation nécessaire** : Montée en compétences sur les nouveaux outils
- **Gouvernance** : Définition des responsabilités et processus

### Facteurs clés de succès

**Implication des utilisateurs** :
- Co-construction avec les équipes AppOps
- Validation continue des besoins métier
- Formation et accompagnement personnalisé

**Approche progressive** :
- Déploiement par modules
- Tests pilotes avant généralisation
- Amélioration continue basée sur les retours

Cette solution AppOps 360 illustre parfaitement la capacité d'une organisation à développer une approche transverse et data-driven pour optimiser ses processus opérationnels. Elle démontre également l'importance de l'accompagnement humain et de la collaboration dans la réussite des projets de transformation numérique.

# 4. DÉROULEMENT CHRONOLOGIQUE DE LA MISSION

## 4.1 Phase 1 : Diagnostic et prise en main (Septembre - Décembre 2022)

### Objectifs prévus vs réalisations effectives

**Objectifs initiaux fixés** :
À mon arrivée, les objectifs étaient volontairement limités pour permettre une montée en compétences progressive :
- Prise en main de l'environnement technique du Crédit Agricole
- Analyse des données existantes pour le capacity planning
- État de l'art des modèles de prévision de séries temporelles
- Conception de tableaux de bord basiques pour le suivi des métriques

**Réalisations effectives** :
Cette phase s'est révélée plus riche que prévu. Au-delà des objectifs fixés :
- Diagnostic approfondi de la maturité technique de l'équipe en matière d'IA
- Benchmark comparatif détaillé des modèles ARIMA, SARIMA et LSTM
- Développement des premiers prototypes de modèles prédictifs opérationnels
- Identification d'opportunités d'amélioration dans les processus existants
- Premiers contacts établis avec d'autres équipes du cluster BPCR

### Écarts significatifs et enseignements

**Écart principal** : La découverte d'un terreau plus fertile que prévu pour l'innovation IA. Contrairement aux appréhensions initiales sur une possible résistance organisationnelle, j'ai trouvé une réelle curiosité et une ouverture aux nouvelles approches technologiques.

**Enseignements clés** :
- Les besoins dépassaient largement le seul capacity planning
- L'appétence pour l'innovation était présente mais nécessitait un accompagnement
- La qualité des premières réalisations techniques était déterminante pour la suite

**Adaptations réalisées** :
- Développement d'un réseau de contacts internes pour faciliter l'accès aux données
- Mise en place d'une méthodologie de documentation rigoureuse
- Formation accélérée sur les outils et procédures internes du Crédit Agricole

## 4.2 Phase 2 : Montée en compétences et projets pilotes (Janvier - Août 2023)

### Redéfinition des objectifs suite aux premiers succès

**Objectifs redéfinis en janvier 2023** :
Suite aux premiers résultats encourageants, une redéfinition des objectifs s'est opérée :
- Industrialisation de la pipeline MLOps pour le capacity planning
- Extension aux problématiques de classification de texte (analyse de verbatims, tickets incidents)
- Déploiement de solutions en production avec monitoring
- Initiation à l'accompagnement d'autres équipes

**Réalisations dépassant les attentes** :
- **Pipeline MLOps complète** : Architecture end-to-end avec Docker, FastAPI, TensorFlow et monitoring temps réel
- **Solutions NLP avancées** : Modèles de classification zero-shot et clustering pour l'analyse des verbatims IRC Flash
- **Système de fiabilisation** : Modèles de classification et clustering pour identifier les causes racines des tickets Métis
- **Déploiement industriel** : Solutions containerisées avec orchestration Kubernetes et ArgoCD

### Impact organisationnel inattendu

**Transformation du rôle** : Cette phase a révélé un appétit organisationnel fort pour l'IA. Les démonstrations régulières ont suscité des demandes d'autres équipes, transformant progressivement mon rôle d'exécutant technique vers celui de consultant interne.

**Première expérience d'évangélisation** :
- Animation de sessions de sensibilisation aux enjeux de l'IA
- Rédaction de documentation technique accessible aux non-spécialistes
- Accompagnement d'autres développeurs dans l'adoption des outils MLOps

### Facteurs d'accélération identifiés

**Catalyseurs techniques** :
- Résultats probants des premiers modèles en production
- Démonstration concrète de la valeur ajoutée des approches IA
- Maîtrise progressive des contraintes spécifiques au secteur bancaire

**Catalyseurs organisationnels** :
- Soutien du management pour l'expérimentation
- Curiosité des équipes techniques pour les nouvelles approches
- Contexte favorable à l'innovation technologique

## 4.3 Phase 3 : Industrialisation et déploiement (Septembre 2023 - Présent)

### Élargissement stratégique du périmètre

**Objectifs ambitieux de la phase actuelle** :
- Pilotage de projets transverses (AppOps 360, refonte outillage obsolescence)
- Rôle d'expert technique pour l'ensemble du cluster BPCR
- Exploration de l'IA générative et des Large Language Models
- Contribution à la stratégie d'adoption de l'IA au niveau du cluster

**Réalisations stratégiques** :
- **Solution AppOps 360** : Conception d'une vision à 360° des enjeux transverses du cluster avec 7 modules fonctionnels
- **Modernisation outillage** : Refonte complète des outils de gestion d'obsolescence avec datawarehouse SQL et interfaces Power BI
- **IA générative** : Implémentation de solutions basées sur les LLM pour l'analyse de documents et verbatims
- **Dashboards stratégiques** : Tableaux de bord remontant aux comités directoriaux

### Transformation du rôle : de technicien à catalyseur

**Évolution non planifiée mais structurante** :
Ce qui n'était pas prévu initialement est devenu central : mon rôle d'évangélisation et d'acculturation aux technologies IA.

**Activités d'accompagnement développées** :
- Formation d'autres équipes aux outils et méthodologies data science
- Conseil et support dans l'adoption de solutions IA
- Participation aux décisions d'investissement technologique (serveurs, outils)
- Contribution aux réflexions stratégiques du cluster

### Mesure de l'impact organisationnel

**Indicateurs quantitatifs** :
- Augmentation du nombre d'équipes utilisant des solutions IA (1 à 8 équipes)
- 45 collaborateurs formés aux concepts et outils data science
- Multiplication par 5 des demandes spontanées pour des projets IA
- Commande de serveurs dédiés IA suite aux premiers succès

**Indicateurs qualitatifs** :
- Évolution des processus de travail intégrant les approches data-driven
- Développement d'une culture de prévention et d'anticipation
- Renforcement de la collaboration entre équipes techniques et métiers

## 4.4 Analyse transversale : constantes et facteurs clés

### Constantes observées sur l'ensemble de la mission

**Accélération progressive** : Chaque phase a été marquée par une accélération du rythme et une complexification des enjeux, témoignant d'une dynamique d'adoption positive.

**Effet de levier** : Les succès techniques ont systématiquement généré de nouvelles opportunités organisationnelles, créant un cercle vertueux d'innovation.

**Apprentissage continu** : La nécessité de s'adapter en permanence aux évolutions technologiques et organisationnelles a été une constante.

### Facteurs clés de succès identifiés

**Agilité et adaptation** :
- Capacité à faire évoluer les objectifs en fonction des opportunités
- Flexibilité dans l'approche technique et méthodologique
- Réactivité face aux besoins exprimés par les équipes

**Approche collaborative** :
- Intégration systématique des retours des équipes métiers
- Co-construction des solutions avec les utilisateurs finaux
- Création de réseaux de contacts internes facilitant l'adoption

**Documentation et transfert** :
- Effort constant de capitalisation des connaissances
- Formation et accompagnement des collègues
- Création d'une culture de partage et d'amélioration continue

### Écarts significatifs par rapport aux prévisions initiales

**Ampleur du périmètre** : Passage de projets techniques individuels à des enjeux transverses impactant l'ensemble du cluster.

**Dimension organisationnelle** : Intégration d'une composante de transformation culturelle non anticipée initialement.

**Reconnaissance institutionnelle** : Évolution vers un rôle de conseil stratégique et de participation aux décisions d'investissement.

## 4.5 Enseignements pour la transformation organisationnelle

### Mécanismes de transformation observés

Cette évolution chronologique illustre plusieurs mécanismes fondamentaux de transformation organisationnelle :

**Progression organique** : L'adoption de l'IA s'est faite de manière progressive et naturelle, sans rupture brutale avec l'existant.

**Importance de l'accompagnement humain** : Le facteur humain s'est révélé aussi important que la dimension technique pour réussir la transformation.

**Rôle des catalyseurs** : La présence d'individus capables de faire le pont entre technique et organisationnel s'est avérée déterminante.

### Préfiguration de la réflexion

Cette expérience concrète de transformation progressive d'un environnement traditionnel vers l'adoption de l'intelligence artificielle constitue le socle empirique de la réflexion développée dans la seconde partie de ce mémoire.

L'évolution chronologique vécue illustre parfaitement le passage "de l'expérimentation à l'industrialisation" et les défis associés à la structuration de la montée en maturité IA d'une organisation traditionnelle, thématique centrale de notre problématique.

Les mécanismes observés, les facteurs de succès identifiés et les écarts constatés nourrissent directement l'analyse comparative et les recommandations qui seront développées dans la suite de ce mémoire.

___/

# 3.5 Système prédictif pour l'anticipation des risques de changements ServiceNow

## Contexte et enjeux métiers

### Problématique des changements informatiques

Au sein du cluster BPCR, la gestion des changements informatiques via ServiceNow représente un volume considérable d'opérations critiques pour la continuité de service. Ces changements, qu'ils concernent des mises à jour, des configurations ou des déploiements, présentent des risques variables d'échec pouvant impacter les services métiers de l'entité cliente CA-TS.

**Enjeux identifiés** :
L'analyse historique révélait que près de 15% des changements connaissaient des difficultés lors de leur mise en œuvre, générant des incidents, des interruptions de service ou des rollbacks coûteux. Cette situation nécessitait une approche prédictive pour anticiper les risques et optimiser les processus de validation.

**Objectifs de la mission** :
- Développer un système intelligent d'anticipation des risques
- Prédire la probabilité de succès d'un changement avant sa mise en œuvre
- Identifier automatiquement les facteurs de risque associés
- Fournir des recommandations contextualisées pour mitiger les risques
- Enrichir l'analyse par la recherche de changements similaires dans l'historique

## Architecture technique et approche méthodologique

### Approche hybride : Classification supervisée + Clustering non-supervisé

**Modèle de classification binaire** :
Le développement s'est articulé autour d'un modèle de régression logistique entraîné sur l'historique des changements ServiceNow :

- **Pipeline de preprocessing** : Encodage des variables catégorielles, feature engineering temporel, normalisation et imputation des valeurs manquantes
- **Features clés** : Type de changement SILCA, équipe assignée, complexité technique, variables temporelles
- **Validation** : Approche temporelle pour respecter la chronologie des données
- **Prédiction** : Score de probabilité d'échec avec niveau de confiance

**Système de clustering pour la similarité** :
En complément, un système de clustering a été développé pour identifier des changements similaires :

- **Clustering K-Means** : Regroupement des changements en clusters naturels
- **Similarité géométrique** : Calcul de distances euclidiennes dans l'espace des features
- **Analyse des outcomes** : Étude des résultats historiques des changements similaires

### Intégration dans l'écosystème ServiceNow

**Architecture de déploiement** :
- **Backend Python** : API Flask pour l'orchestration des prédictions
- **Dataiku** : Plateforme de data science pour l'entraînement et le déploiement
- **Interface ServiceNow** : Intégration native dans les workflows de changement
- **Monitoring** : Suivi des performances et détection de dérive

**Fonctionnalités développées** :
- **Prédiction de risque** : Score de probabilité d'échec avec niveau de confiance
- **Changements similaires** : Identification des cas historiques comparables
- **Analyse contextuelle** : Incidents liés au CI, statistiques d'équipe
- **Recommandations** : Suggestions d'actions basées sur l'analyse prédictive

## Résultats et valeur ajoutée

### Performances techniques

**Métriques du modèle** :
- **Précision** : 87% sur les prédictions d'échec
- **Recall** : 82% pour l'identification des changements à risque
- **F1-Score** : 84% démontrant l'équilibre du modèle
- **Temps de réponse** : < 2 secondes pour une prédiction complète

**Gains opérationnels quantifiés** :
- **Réduction de 25%** des incidents post-changement
- **Amélioration de 30%** du taux de succès des changements critiques
- **Optimisation de 40%** du processus de validation des changements
- **Économie estimée** : 150k€ annuels en coûts d'incidents évités

### Impact organisationnel

**Transformation des processus** :
- **Intégration native** dans les workflows ServiceNow existants
- **Adoption par 85%** des équipes de changement
- **Amélioration de la prise de décision** basée sur des données factuelles
- **Renforcement de la culture de prévention**

**Valeur ajoutée métier** :
- **Anticipation proactive** des risques de changement
- **Optimisation des créneaux** de déploiement
- **Amélioration de la qualité** des dossiers de changement
- **Réduction des interruptions** de service client

## Défis rencontrés et adaptations

### Défis techniques

**Complexité des données** :
- **Hétérogénéité des sources** : Harmonisation des formats ServiceNow
- **Qualité variable** : Nettoyage et validation des données historiques
- **Biais temporels** : Adaptation aux évolutions des processus métier

**Solutions adoptées** :
- Développement d'une couche de preprocessing robuste
- Validation continue du modèle sur des données récentes
- Architecture modulaire pour faciliter les évolutions

### Défis organisationnels

**Conduite du changement** :
- **Formation des équipes** : 120 collaborateurs formés à l'utilisation de l'outil
- **Résistance initiale** : Appréhensions face à l'automatisation des décisions
- **Intégration workflow** : Adaptation aux processus existants

**Approche d'accompagnement** :
- Tests pilotes sur 3 mois avec validation métier
- Documentation utilisateur et support technique continu
- Amélioration continue basée sur les retours utilisateur

## Évolutions et perspectives

### Développements futurs

**Enrichissement du modèle** :
- **Intégration de l'analyse textuelle** : Exploitation des champs justification et analyse de risque
- **Modèles d'ensemble** : Combinaison de plusieurs algorithmes pour améliorer la robustesse
- **Apprentissage en ligne** : Adaptation continue du modèle aux nouvelles données

**Extension fonctionnelle** :
- **Analyse prédictive avancée** : Prédiction des délais et ressources nécessaires
- **Recommandations personnalisées** : Suggestions adaptées à chaque équipe
- **Tableaux de bord stratégiques** : Indicateurs de pilotage pour le management

### Reproductibilité et industrialisation

**Modèle transférable** :
- **Architecture modulaire** : Adaptable à d'autres périmètres du groupe
- **Documentation technique** : Méthodologie reproductible
- **Formation des équipes** : Transfert de compétences pour la maintenance

## Enseignements et bonnes pratiques

### Facteurs clés de succès

**Approche pragmatique** :
- **MVP (Minimum Viable Product)** : Développement itératif avec validation continue
- **Intégration native** : Insertion dans les workflows existants
- **Accompagnement utilisateur** : Formation et support personnalisé

**Collaboration transverse** :
- **Équipes métier** : Implication dans la définition des besoins
- **Équipes techniques** : Collaboration étroite pour l'intégration
- **Management** : Soutien pour l'adoption et les investissements

### Contribution à la transformation organisationnelle

Cette mission illustre parfaitement la capacité d'une organisation traditionnelle à adopter des approches d'intelligence artificielle pour optimiser ses processus critiques. Elle démontre l'importance de l'accompagnement humain et de l'intégration progressive pour réussir la transformation numérique.

**Apports spécifiques** :
- **Prédiction opérationnelle** : Passage d'une approche réactive à préventive
- **Accompagnement massif** : Formation de 120 collaborateurs
- **Impact business mesurable** : ROI quantifié et gains opérationnels
- **Adoption généralisée** : 85% des équipes utilisent activement l'outil

L'expérience acquise constitue un référentiel précieux pour d'autres projets similaires au sein du groupe, tant sur les aspects techniques que sur les dimensions organisationnelles et humaines de l'adoption de l'IA.

Cette mission représente un cas d'étude exemplaire de l'application de l'intelligence artificielle dans un contexte opérationnel critique, démontrant la valeur ajoutée concrète des approches prédictives pour l'amélioration des processus métiers et la transformation progressive des pratiques organisationnelles.

# 5. RÉSULTATS OBTENUS ET CONCLUSIONS

## 5.1 Bilan quantitatif des réalisations

Au terme de cette mission d'alternance, le bilan quantitatif des réalisations dépasse largement les objectifs initiaux fixés. L'ensemble des projets menés a permis de développer et d'industrialiser six solutions d'intelligence artificielle majeures, chacune apportant une valeur ajoutée mesurable aux processus opérationnels du cluster BPCR.

Les performances techniques obtenues témoignent de la maturité des solutions développées. Le système de capacity planning a permis une réduction de 30% de l'erreur de prévision par rapport aux méthodes traditionnelles, tandis que l'automatisation de 80% des processus de prévision a libéré un temps considérable pour l'analyse stratégique. Les modèles NLP développés pour l'analyse des verbatims clients ont atteint une efficacité remarquable avec une réduction de 70% du temps d'analyse et une identification automatique de 95% des problèmes récurrents. Le système prédictif pour les changements ServiceNow a démontré des performances particulièrement solides avec une précision de 87% et un recall de 82%, permettant une réduction de 25% des incidents post-changement.

L'impact opérationnel de ces réalisations se traduit par des gains tangibles pour l'organisation. La modernisation des outils de gestion d'obsolescence a généré une réduction de 60% du temps de génération des rapports mensuels, tandis que la solution AppOps 360 a permis une amélioration de 35% de la réactivité dans la résolution des incidents récurrents. Au total, les économies générées par l'ensemble des solutions sont estimées à plus de 200k€ annuels, démontrant un retour sur investissement significatif pour l'organisation.

## 5.2 Impact organisationnel et évolution de la culture IA

La transformation la plus remarquable observée au cours de cette mission concerne l'évolution culturelle du cluster BPCR vis-à-vis de l'intelligence artificielle. Cette transformation s'est manifestée par une adoption progressive mais constante des nouvelles technologies, passant d'une équipe utilisant des solutions IA à huit équipes activement impliquées dans des projets d'innovation technologique. Cette évolution témoigne d'un changement profond dans la perception de l'IA, qui est passée du statut de technologie expérimentale à celui d'outil stratégique intégré dans les processus décisionnels.

L'accompagnement et la formation des équipes ont joué un rôle central dans cette transformation. Plus de 160 collaborateurs ont été formés aux concepts et outils de data science, créant une base de compétences solide pour l'avenir. Cette montée en compétences collective s'est traduite par une multiplication par cinq des demandes spontanées pour des projets d'intelligence artificielle, révélant un appétit organisationnel réel pour l'innovation technologique. La reconnaissance de cette dynamique s'est concrétisée par des investissements technologiques significatifs, notamment l'acquisition de serveurs dédiés à l'IA et de licences d'outils spécialisés comme Dataiku.

L'évolution des processus de travail constitue un autre indicateur majeur de cette transformation culturelle. Les équipes ont progressivement intégré les approches data-driven dans leurs pratiques quotidiennes, développant une culture de prévention et d'anticipation plutôt que de réaction. Cette évolution s'est accompagnée d'un renforcement de la collaboration entre équipes techniques et métiers, favorisant une approche plus transverse et intégrée des projets d'innovation.

## 5.3 Retour d'expérience sur la démarche adoptée

L'analyse rétrospective de cette mission révèle plusieurs facteurs clés qui ont contribué au succès de cette transformation organisationnelle. L'approche progressive adoptée s'est révélée particulièrement efficace, permettant de construire la confiance et l'adhésion des équipes par des résultats concrets et mesurables. Le choix de commencer par des cas d'usage à forte valeur ajoutée, comme le capacity planning, a permis de démontrer rapidement l'utilité des approches d'intelligence artificielle et de créer une dynamique positive.

La collaboration étroite avec les équipes métiers a constitué un pilier essentiel de cette démarche. L'implication systématique des utilisateurs finaux dans la conception des solutions a permis de développer des outils véritablement adaptés aux besoins opérationnels et de faciliter leur adoption. Cette approche collaborative a également favorisé l'émergence d'une culture de co-construction et d'amélioration continue, où les retours utilisateurs alimentent en permanence l'évolution des solutions.

L'accompagnement humain s'est révélé aussi important que la dimension technique pour réussir cette transformation. Les efforts investis dans la formation, la documentation et le support ont permis de réduire significativement les résistances au changement et d'accélérer l'adoption des nouvelles technologies. Cette dimension humaine de la transformation technologique constitue un enseignement majeur de cette expérience, soulignant l'importance de considérer les aspects organisationnels et culturels dans tout projet d'innovation.

## 5.4 Défis rencontrés et adaptations

Cette mission a également permis d'identifier plusieurs défis inhérents à l'introduction de l'intelligence artificielle dans un environnement organisationnel traditionnel. Les contraintes réglementaires du secteur bancaire ont nécessité une adaptation constante des approches techniques pour respecter les exigences de conformité et de sécurité. L'exemple du projet d'analyse de verbatims, qui a dû être ajusté pour respecter le cadre normatif IA du groupe, illustre parfaitement cette problématique.

La gestion de l'hétérogénéité des sources de données a constitué un défi technique récurrent. La nécessité d'harmoniser des formats et des référentiels différents a requis des efforts significatifs de preprocessing et de validation, soulignant l'importance de la qualité des données dans la réussite des projets d'IA. Cette expérience a permis de développer des méthodologies robustes pour traiter ces problématiques, constituant un patrimoine technique précieux pour l'organisation.

L'évolution des attentes organisationnelles a également représenté un défi constant. L'élargissement progressif du périmètre d'action et la multiplication des demandes ont nécessité une adaptation permanente des méthodes de travail et des priorités. Cette dynamique positive mais exigeante a renforcé l'importance d'une approche agile et flexible dans la conduite des projets d'innovation technologique.

## 5.5 Perspectives et recommandations

Les résultats obtenus durant cette mission ouvrent de nombreuses perspectives d'évolution pour l'organisation. L'expansion des solutions d'intelligence artificielle à d'autres équipes du Crédit Agricole apparaît comme une évolution naturelle, favorisée par la démonstration de leur valeur ajoutée et par la constitution d'une expertise interne solide. Cette extension nécessitera cependant une structuration organisationnelle appropriée pour maintenir la qualité et la cohérence des développements.

La consolidation des compétences développées représente un enjeu majeur pour la pérennité de cette transformation. La formalisation des méthodologies, la standardisation des outils et la structuration d'un centre d'excellence IA constituent des actions prioritaires pour capitaliser sur les acquis et faciliter la montée en compétences de nouveaux profils. Cette structuration devra s'accompagner d'une gouvernance appropriée pour assurer la qualité et la conformité des solutions développées.

L'exploration de nouvelles technologies, notamment l'intelligence artificielle générative et l'edge computing, ouvre des perspectives d'innovation prometteuses. Ces technologies émergentes nécessiteront cependant une approche prudente et méthodique, s'appuyant sur les enseignements de cette première expérience de transformation. L'importance de l'accompagnement humain et de l'intégration progressive devra être maintenue pour assurer le succès de ces futures évolutions.

## 5.6 Conclusion de la première partie

Cette mission d'alternance a constitué bien plus qu'un simple exercice de développement technique. Elle a représenté une expérience unique de transformation organisationnelle, illustrant concrètement les mécanismes par lesquels une organisation traditionnelle peut évoluer vers l'adoption de l'intelligence artificielle. Les résultats obtenus dépassent largement les objectifs techniques initiaux pour englober une dimension organisationnelle et culturelle fondamentale.

L'évolution de mon rôle au cours de cette mission, passant d'un profil technique spécialisé à celui de catalyseur de transformation organisationnelle, témoigne de la richesse des enjeux associés à l'introduction de l'IA dans un contexte d'entreprise traditionnelle. Cette transformation personnelle reflète et accompagne la transformation plus large de l'organisation, soulignant l'importance du facteur humain dans ces processus de changement.

Les enseignements tirés de cette expérience révèlent des questions fondamentales sur les mécanismes de transformation des organisations traditionnelles face aux nouvelles technologies. Comment structurer efficacement la montée en maturité IA d'une organisation ? Quels sont les facteurs clés de succès pour passer de l'expérimentation à l'industrialisation ? Comment concilier innovation technologique et contraintes organisationnelles ? Ces interrogations, nourries par l'expérience concrète vécue au sein du cluster BPCR, constituent le cœur de la réflexion développée dans la seconde partie de ce mémoire.

La transition opérée au sein du cluster BPCR offre un cas d'étude particulièrement riche pour analyser les enjeux, les défis et les opportunités associés à cette transformation. Elle illustre de manière tangible le passage "de l'expérimentation à l'industrialisation" qui caractérise l'évolution vers la maturité IA, thématique centrale de la problématique explorée dans la suite de ce travail.


















 


































