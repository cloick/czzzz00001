L'utilisateur veut maintenant les codes des fichiers corrig√©s avec l'approche "mod√®le pr√©-entra√Æn√©" que j'ai recommand√©e. Je dois donc modifier les fichiers pour :

1. Retirer la m√©thode train_model() des Code Libraries
2. Ajouter une m√©thode load_pretrained_model() qui charge un mod√®le d√©j√† entra√Æn√©
3. Utiliser les vrais param√®tres optimaux bay√©siens qu'il m'a donn√©s
4. Corriger les erreurs que j'avais faites (feature_importances_ avec penalty='l1')
5. Simplifier l'architecture pour chargement rapide

Rappel des param√®tres optimaux de son mod√®le :
- C: 105.76211650904162
- penalty: 'l1'  
- solver: 'liblinear'
- max_iter: 2708
- class_weight: 'balanced'
- random_state: 42

Et les 5 features s√©lectionn√©es :
['dv_u_type_change_silca', 'dv_type', 'u_cab_count', 'u_bcr', 'u_bpc']

## üìÅ **FICHIERS CORRIG√âS - VERSION MOD√àLE PR√â-ENTRA√éN√â**

### **FICHIER 1: change_risk_predictor.py**

```python
"""
Module principal pour la pr√©diction des risques de changement
Mod√®le pr√©-entra√Æn√© pour chargement rapide
"""

import pandas as pd
import numpy as np
import joblib
from datetime import datetime

class ChangeRiskPredictor:
    """Pr√©dicteur de risque pour changements ServiceNow - Version pr√©-entra√Æn√©e"""
    
    def __init__(self):
        self.model = None
        self.selector = None
        self.feature_names = ['dv_u_type_change_silca', 'dv_type', 'u_cab_count', 'u_bcr', 'u_bpc']
        self.is_loaded = False
        
        # Chargement automatique au d√©marrage
        self.load_pretrained_model()
        
    def load_pretrained_model(self):
        """Charger le mod√®le et selector pr√©-entra√Æn√©s"""
        
        try:
            print("üîÑ Chargement du mod√®le pr√©-entra√Æn√©...")
            
            # TODO: Adapter le chemin selon votre configuration Dataiku
            # Option 1: Depuis managed folder
            # folder = dataiku.Folder("change_risk_models")
            # with folder.get_reader("model_final.pkl") as reader:
            #     self.model = joblib.load(reader)
            
            # Option 2: Depuis code libraries (temporaire)
            # self.model = joblib.load('model_final.pkl')
            # self.selector = joblib.load('selector_final.pkl')
            
            # Pour l'instant, on simule le chargement
            # Vous remplacerez par le vrai chargement apr√®s sauvegarde
            print("‚ö†Ô∏è Mod√®le pas encore sauvegard√© - simulation temporaire")
            self.is_loaded = False
            
        except Exception as e:
            print(f"‚ùå Erreur chargement mod√®le: {e}")
            print("üí° Assurez-vous que le mod√®le a √©t√© sauvegard√© dans le notebook final")
            self.is_loaded = False
    
    def predict_risk_score(self, change_data):
        """
        Pr√©dire le score de risque d'un changement
        Retourne un pourcentage (0-100%)
        """
        
        if not self.is_loaded:
            raise ValueError("‚ùå Mod√®le non charg√©. V√©rifiez la sauvegarde du mod√®le.")
        
        # Preprocessing
        change_features = self._prepare_single_change(change_data)
        change_selected = self.selector.transform(change_features)
        
        # Probabilit√© de probl√®me (classe 0)
        risk_probability = self.model.predict_proba(change_selected)[0, 0]
        
        # Convertir en pourcentage
        risk_score = risk_probability * 100
        
        return round(risk_score, 1)
    
    def get_detailed_analysis(self, change_data):
        """Analyse compl√®te d'un changement"""
        
        # Score de risque
        risk_score = self.predict_risk_score(change_data)
        
        # Niveau de risque
        risk_level = self._get_risk_level(risk_score)
        
        # Facteurs de risque d√©tect√©s
        risk_factors = self._analyze_risk_factors(change_data)
        
        # Recommandations
        recommendations = self._get_recommendations(risk_level['level'])
        
        return {
            'risk_score': risk_score,
            'risk_level': risk_level['level'],
            'risk_color': risk_level['color'],
            'interpretation': risk_level['interpretation'],
            'risk_factors': risk_factors,
            'recommendations': recommendations,
            'model_confidence': 'Mod√©r√©e (53% recall, 14% precision)',
            'model_info': 'LogisticRegression optimis√© bay√©sien'
        }
    
    def _get_risk_level(self, risk_score):
        """D√©terminer le niveau de risque"""
        
        if risk_score >= 75:
            return {
                'level': '√âLEV√â',
                'color': 'üî¥',
                'interpretation': 'Probabilit√© d\'√©chec importante'
            }
        elif risk_score >= 50:
            return {
                'level': 'MOYEN',
                'color': 'üü°', 
                'interpretation': 'Risque de complications possibles'
            }
        else:
            return {
                'level': 'FAIBLE',
                'color': 'üü¢',
                'interpretation': 'Profil de changement standard'
            }
    
    def _analyze_risk_factors(self, change_data):
        """Analyser les facteurs de risque bas√©s sur les 5 features r√©elles"""
        
        risk_factors = []
        
        # Extraire les valeurs des 5 features
        raw_values = {
            'dv_u_type_change_silca': change_data.get('dv_u_type_change_silca'),
            'dv_type': change_data.get('dv_type'),
            'u_cab_count': change_data.get('u_cab_count'),
            'u_bcr': change_data.get('u_bcr'),
            'u_bpc': change_data.get('u_bpc')
        }
        
        # Analyse bas√©e sur vos d√©couvertes d'exploration
        
        # 1. Type SILCA (votre top variable cat√©gorielle)
        if raw_values['dv_u_type_change_silca'] == 'Complex':
            risk_factors.append("Type de changement SILCA complexe")
        
        # 2. Type de changement
        if raw_values['dv_type'] in ['Urgent', 'Emergency']:
            risk_factors.append(f"Type de changement √† risque ({raw_values['dv_type']})")
        
        # 3. Nombre de CAB
        if raw_values['u_cab_count'] and raw_values['u_cab_count'] >= 3:
            risk_factors.append(f"Nombre √©lev√© de CAB requis ({raw_values['u_cab_count']})")
        
        # 4. P√©rim√®tre BCR
        if raw_values['u_bcr'] == True:
            risk_factors.append("P√©rim√®tre BCR impact√©")
        
        # 5. P√©rim√®tre BPC
        if raw_values['u_bpc'] == True:
            risk_factors.append("P√©rim√®tre BPC impact√©")
        
        return risk_factors
    
    def _get_recommendations(self, risk_level):
        """Recommandations selon le niveau de risque"""
        
        recommendations = {
            '√âLEV√â': [
                "R√©vision CAB recommand√©e",
                "Plan de rollback d√©taill√© requis", 
                "Tests approfondis conseill√©s",
                "Surveillance post-d√©ploiement renforc√©e"
            ],
            'MOYEN': [
                "Surveillance renforc√©e conseill√©e",
                "V√©rification des pr√©requis",
                "Communication √©quipe √©tendue",
                "Documentation des √©tapes critiques"
            ],
            'FAIBLE': [
                "Proc√©dure standard applicable",
                "Surveillance normale",
                "Documentation standard"
            ]
        }
        
        return recommendations.get(risk_level, [])
    
    def _prepare_single_change(self, change_data):
        """Pr√©parer les donn√©es d'un changement unique pour pr√©diction"""
        
        # Convertir en DataFrame si n√©cessaire
        if isinstance(change_data, dict):
            change_df = pd.DataFrame([change_data])
        else:
            change_df = change_data.copy()
        
        return change_df
    
    def get_model_info(self):
        """Informations sur le mod√®le charg√©"""
        
        if not self.is_loaded:
            return {"status": "Mod√®le non charg√©"}
        
        return {
            "status": "Mod√®le charg√©",
            "algorithm": "LogisticRegression",
            "optimization": "Bayesian (scikit-optimize)",
            "features": self.feature_names,
            "performance": {
                "recall": "53.1%",
                "precision": "14.2%", 
                "f1": "22.6%"
            },
            "hyperparameters": {
                "C": 105.76211650904162,
                "penalty": "l1",
                "solver": "liblinear",
                "max_iter": 2708
            }
        }
```

### **FICHIER 2: servicenow_connector.py**

```python
"""
Connecteur pour r√©cup√©rer les donn√©es ServiceNow et informations d'enrichissement
Version simplifi√©e pour mod√®le pr√©-entra√Æn√©
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re

class ServiceNowConnector:
    """Connecteur pour donn√©es ServiceNow et enrichissement"""
    
    def __init__(self):
        # Configuration des connexions (√† adapter selon votre environnement Dataiku)
        self.connection_status = "Simul√©"  # √Ä remplacer par vraie connexion
        
    def get_change_data(self, change_ref):
        """
        R√©cup√©rer les donn√©es d'un changement sp√©cifique
        
        Args:
            change_ref (str): R√©f√©rence du changement (ex: CHG001234)
            
        Returns:
            dict: Donn√©es du changement ou None si non trouv√©
        """
        
        # Validation du format
        if not self.validate_change_reference(change_ref):
            print(f"‚ùå Format de r√©f√©rence invalide: {change_ref}")
            return None
        
        try:
            # TODO: Remplacer par vraie requ√™te vers Snow Mirror
            # query = f"""
            # SELECT dv_u_type_change_silca, dv_type, u_cab_count, u_bcr, u_bpc,
            #        dv_assignment_group, dv_cmdb_ci, short_description, opened_at
            # FROM change_request 
            # WHERE number = '{change_ref}'
            # """
            
            # En attendant, simulation avec des donn√©es factices coh√©rentes
            change_data = {
                'number': change_ref,
                
                # === 5 FEATURES EXACTES DU MOD√àLE ===
                'dv_u_type_change_silca': np.random.choice(['Simple', 'Complex'], p=[0.7, 0.3]),
                'dv_type': np.random.choice(['Normal', 'Urgent', 'Emergency'], p=[0.8, 0.15, 0.05]),
                'u_cab_count': np.random.randint(0, 5),
                'u_bcr': np.random.choice([True, False], p=[0.3, 0.7]),
                'u_bpc': np.random.choice([True, False], p=[0.2, 0.8]),
                
                # === DONN√âES D'ENRICHISSEMENT ===
                'dv_assignment_group': np.random.choice([
                    '√âquipe Infrastructure', '√âquipe Application', 
                    '√âquipe R√©seau', '√âquipe S√©curit√©'
                ]),
                'dv_cmdb_ci': f'Server-{np.random.choice(["PROD", "PREP", "DEV"])}-{np.random.randint(100, 999):03d}',
                'dv_category': np.random.choice(['Infrastructure', 'Application', 'R√©seau']),
                'opened_at': datetime.now() - timedelta(days=np.random.randint(0, 3)),
                'short_description': f'Changement de maintenance {change_ref}',
                'dv_impact': np.random.choice(['Faible', 'Moyen', '√âlev√©'], p=[0.6, 0.3, 0.1])
            }
            
            print(f"‚úÖ Changement {change_ref} r√©cup√©r√©")
            return change_data
            
        except Exception as e:
            print(f"‚ùå Erreur r√©cup√©ration changement {change_ref}: {e}")
            return None
    
    def get_team_statistics(self, assignment_group, months_back=6):
        """
        R√©cup√©rer les statistiques d'une √©quipe
        
        Args:
            assignment_group (str): Nom de l'√©quipe
            months_back (int): Nombre de mois √† analyser
        """
        
        try:
            # TODO: Vraie requ√™te SQL vers Snow Mirror
            # query = f"""
            # SELECT COUNT(*) as total,
            #        SUM(CASE WHEN dv_close_code = 'Succ√®s' THEN 1 ELSE 0 END) as successes
            # FROM change_request 
            # WHERE dv_assignment_group = '{assignment_group}' 
            # AND opened_at >= DATE_SUB(NOW(), INTERVAL {months_back} MONTH)
            # """
            
            # Simulation bas√©e sur des patterns r√©alistes
            total_changes = np.random.randint(20, 150)
            
            # Certaines √©quipes plus fiables que d'autres
            if 'Infrastructure' in assignment_group:
                success_rate = np.random.uniform(0.85, 0.95)
            elif 'Application' in assignment_group:
                success_rate = np.random.uniform(0.75, 0.90)
            else:
                success_rate = np.random.uniform(0.70, 0.85)
            
            successes = int(total_changes * success_rate)
            failures = total_changes - successes
            
            team_stats = {
                'assignment_group': assignment_group,
                'period_months': months_back,
                'total_changes': total_changes,
                'successes': successes,
                'failures': failures,
                'success_rate': round(success_rate * 100, 1),
                'last_failure_date': datetime.now() - timedelta(days=np.random.randint(1, 30)) if failures > 0 else None,
                'trend': 'stable'  # TODO: Calculer vraie tendance
            }
            
            print(f"‚úÖ Statistiques √©quipe {assignment_group} r√©cup√©r√©es")
            return team_stats
            
        except Exception as e:
            print(f"‚ùå Erreur stats √©quipe {assignment_group}: {e}")
            return None
    
    def get_solution_incidents(self, cmdb_ci, months_back=3):
        """
        R√©cup√©rer les incidents li√©s √† une solution/CI
        """
        
        try:
            # TODO: Requ√™te vers table incidents ServiceNow
            
            # Simulation r√©aliste
            incident_count = np.random.poisson(2)  # Distribution de Poisson
            
            incidents_data = {
                'cmdb_ci': cmdb_ci,
                'period_months': months_back,
                'total_incidents': incident_count,
                'critical_incidents': max(0, incident_count - np.random.randint(0, 2)),
                'avg_resolution_hours': np.random.randint(1, 24) if incident_count > 0 else 0,
                'last_incident_date': datetime.now() - timedelta(days=np.random.randint(1, 90)) if incident_count > 0 else None,
                'incident_types': ['Performance', 'Disponibilit√©', 'S√©curit√©'][:incident_count]
            }
            
            print(f"‚úÖ Incidents {cmdb_ci} r√©cup√©r√©s")
            return incidents_data
            
        except Exception as e:
            print(f"‚ùå Erreur incidents {cmdb_ci}: {e}")
            return None
    
    def find_similar_changes(self, change_data, limit=10):
        """
        Trouver les changements similaires bas√©s sur r√®gles m√©tier
        Bas√© sur vos d√©couvertes d'analyse exploratoire
        """
        
        try:
            # Crit√®res de similarit√© (bas√©s sur vos variables importantes)
            base_criteria = {
                'dv_u_type_change_silca': change_data.get('dv_u_type_change_silca'),
                'dv_type': change_data.get('dv_type'),
                'dv_assignment_group': change_data.get('dv_assignment_group'),
                'dv_category': change_data.get('dv_category')
            }
            
            # TODO: Vraie requ√™te SQL avec scoring de similarit√©
            
            # Simulation de changements similaires avec distribution r√©aliste
            similar_changes = []
            
            for i in range(min(limit, np.random.randint(3, 8))):
                
                # Simuler r√©sultats avec distribution proche de la r√©alit√© (90% succ√®s)
                close_code = np.random.choice([
                    'Succ√®s', 'Succ√®s avec difficult√©s', 'Impl√©ment√© partiellement',
                    '√âchec avec retour arri√®re', '√âchec sans retour arri√®re'
                ], p=[0.896, 0.054, 0.021, 0.016, 0.013])  # Vos vraies distributions
                
                similar_change = {
                    'number': f'CHG{np.random.randint(1000000, 9999999):07d}',
                    'dv_close_code': close_code,
                    'short_description': f'Changement similaire - {base_criteria["dv_type"]} sur {base_criteria["dv_u_type_change_silca"]}',
                    'opened_at': datetime.now() - timedelta(days=np.random.randint(30, 365)),
                    'similarity_score': np.random.randint(60, 95),
                    'close_notes': self._generate_close_notes(close_code),
                    'assignment_group': change_data.get('dv_assignment_group'),
                    'duration_hours': np.random.randint(1, 8)
                }
                
                similar_changes.append(similar_change)
            
            # Trier par score de similarit√©
            similar_changes.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            print(f"‚úÖ {len(similar_changes)} changements similaires trouv√©s")
            return similar_changes
            
        except Exception as e:
            print(f"‚ùå Erreur changements similaires: {e}")
            return []
    
    def _generate_close_notes(self, close_code):
        """G√©n√©rer des notes de fermeture r√©alistes"""
        
        notes_templates = {
            'Succ√®s': [
                'Changement appliqu√© avec succ√®s. Aucun incident d√©tect√©.',
                'D√©ploiement r√©alis√© conform√©ment au planning. Tests OK.',
                'Migration termin√©e sans probl√®me. Services op√©rationnels.'
            ],
            'Succ√®s avec difficult√©s': [
                'Changement r√©alis√© mais quelques difficult√©s mineures rencontr√©es.',
                'D√©ploiement ok apr√®s r√©solution probl√®me configuration.',
                'Succ√®s final apr√®s ajustement proc√©dure.'
            ],
            '√âchec avec retour arri√®re': [
                '√âchec d√©tect√©, rollback effectu√© avec succ√®s.',
                'Probl√®me critique, retour √† l\'√©tat ant√©rieur.',
                'Changement annul√©, syst√®me restaur√©.'
            ],
            '√âchec sans retour arri√®re': [
                '√âchec partiel, correction manuelle appliqu√©e.',
                'Probl√®me r√©solu par intervention d\'urgence.',
                'Echec mineur, service maintenu.'
            ],
            'Impl√©ment√© partiellement': [
                'Changement partiellement r√©alis√© selon planning.',
                'Phase 1 termin√©e, phase 2 report√©e.',
                'Impl√©mentation partielle conforme aux objectifs.'
            ]
        }
        
        templates = notes_templates.get(close_code, ['Notes de fermeture standard.'])
        return np.random.choice(templates)
    
    def validate_change_reference(self, change_ref):
        """Valider le format de la r√©f√©rence changement ServiceNow"""
        
        # Format ServiceNow standard: CHG + 7 chiffres
        pattern = r'^CHG\d{7}$'
        
        if re.match(pattern, change_ref):
            return True
        else:
            return False
    
    def get_connection_status(self):
        """V√©rifier le statut de connexion Snow Mirror"""
        
        return {
            'status': self.connection_status,
            'last_sync': datetime.now() - timedelta(hours=1),
            'available_tables': ['change_request', 'incident', 'cmdb_ci']
        }
```

### **FICHIER 3: data_preprocessing.py**

```python
"""
Module pour le preprocessing des donn√©es de changement
Version simplifi√©e pour mod√®le pr√©-entra√Æn√©
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

class ChangeDataPreprocessor:
    """Preprocessing des donn√©es de changement - Version pr√©-entra√Æn√©e"""
    
    def __init__(self):
        # Les encoders seront charg√©s avec le mod√®le pr√©-entra√Æn√©
        self.label_encoders = {}
        self.is_fitted = False
        
    def load_fitted_preprocessor(self, encoders_dict):
        """Charger les encoders pr√©-entra√Æn√©s"""
        
        self.label_encoders = encoders_dict
        self.is_fitted = True
        print("‚úÖ Preprocessor pr√©-entra√Æn√© charg√©")
        
    def transform_single_change(self, change_data):
        """
        Transformer un changement unique (pour pr√©diction en temps r√©el)
        Optimis√© pour l'utilisation dans la webapp
        """
        
        # Convertir en DataFrame
        if isinstance(change_data, dict):
            df = pd.DataFrame([change_data])
        else:
            df = change_data.copy()
        
        # 1. Garder seulement les 5 features du mod√®le
        required_features = ['dv_u_type_change_silca', 'dv_type', 'u_cab_count', 'u_bcr', 'u_bpc']
        
        # V√©rifier que toutes les features sont pr√©sentes
        for feature in required_features:
            if feature not in df.columns:
                print(f"‚ö†Ô∏è Feature manquante: {feature}")
                df[feature] = 0  # Valeur par d√©faut
        
        # Garder seulement les features n√©cessaires
        df_features = df[required_features].copy()
        
        # 2. Encodage des variables cat√©gorielles
        df_encoded = self._encode_categorical_single(df_features)
        
        # 3. Imputation des valeurs manquantes
        df_final = self._imputation_single(df_encoded)
        
        return df_final
    
    def _encode_categorical_single(self, df):
        """Encoder les variables cat√©gorielles pour un changement unique"""
        
        df_encoded = df.copy()
        
        # Mapping des valeurs bas√© sur vos donn√©es d'entra√Ænement
        # TODO: Ces mappings doivent √™tre sauvegard√©s avec le mod√®le pr√©-entra√Æn√©
        
        default_encodings = {
            'dv_u_type_change_silca': {'Simple': 0, 'Complex': 1},
            'dv_type': {'Normal': 0, 'Urgent': 1, 'Emergency': 2}
        }
        
        for col in ['dv_u_type_change_silca', 'dv_type']:
            if col in df_encoded.columns:
                if col in default_encodings:
                    # Utiliser le mapping par d√©faut
                    mapping = default_encodings[col]
                    df_encoded[col] = df_encoded[col].map(mapping).fillna(0)
                else:
                    # Encoder num√©riquement si nouvelle valeur
                    df_encoded[col] = pd.Categorical(df_encoded[col]).codes
        
        return df_encoded
    
    def _imputation_single(self, df):
        """Imputation des valeurs manquantes pour un changement unique"""
        
        df_imputed = df.copy()
        
        # Imputation avec valeurs par d√©faut bas√©es sur vos analyses
        default_values = {
            'dv_u_type_change_silca': 0,  # Simple (valeur la plus fr√©quente)
            'dv_type': 0,                 # Normal (valeur la plus fr√©quente)
            'u_cab_count': 1,             # Valeur m√©diane typique
            'u_bcr': 0,                   # False encod√© (plus fr√©quent)
            'u_bpc': 0                    # False encod√© (plus fr√©quent)
        }
        
        for col, default_val in default_values.items():
            if col in df_imputed.columns:
                df_imputed[col].fillna(default_val, inplace=True)
        
        # S'assurer que tout est num√©rique
        for col in df_imputed.columns:
            df_imputed[col] = pd.to_numeric(df_imputed[col], errors='coerce').fillna(0)
        
        return df_imputed
    
    def validate_input_data(self, change_data):
        """Valider les donn√©es d'entr√©e"""
        
        required_features = ['dv_u_type_change_silca', 'dv_type', 'u_cab_count', 'u_bcr', 'u_bpc']
        
        validation_results = {
            'is_valid': True,
            'missing_features': [],
            'invalid_values': [],
            'warnings': []
        }
        
        # V√©rifier pr√©sence des features
        for feature in required_features:
            if feature not in change_data:
                validation_results['missing_features'].append(feature)
                validation_results['warnings'].append(f"Feature {feature} manquante, valeur par d√©faut utilis√©e")
        
        # V√©rifier types et valeurs
        if 'u_cab_count' in change_data:
            if not isinstance(change_data['u_cab_count'], (int, float)) or change_data['u_cab_count'] < 0:
                validation_results['invalid_values'].append('u_cab_count doit √™tre un nombre positif')
        
        if 'dv_u_type_change_silca' in change_data:
            if change_data['dv_u_type_change_silca'] not in ['Simple', 'Complex']:
                validation_results['warnings'].append('Type SILCA inconnu, trait√© comme Simple')
        
        if 'dv_type' in change_data:
            if change_data['dv_type'] not in ['Normal', 'Urgent', 'Emergency']:
                validation_results['warnings'].append('Type de changement inconnu, trait√© comme Normal')
        
        return validation_results
    
    def get_feature_summary(self, processed_data):
        """R√©sum√© des features apr√®s preprocessing"""
        
        if isinstance(processed_data, pd.DataFrame) and len(processed_data) > 0:
            row = processed_data.iloc[0]
            
            # D√©coder les valeurs pour affichage
            silca_types = {0: 'Simple', 1: 'Complex'}
            change_types = {0: 'Normal', 1: 'Urgent', 2: 'Emergency'}
            
            summary = {
                'dv_u_type_change_silca': silca_types.get(row['dv_u_type_change_silca'], 'Inconnu'),
                'dv_type': change_types.get(row['dv_type'], 'Inconnu'),
                'u_cab_count': int(row['u_cab_count']),
                'u_bcr': bool(row['u_bcr']),
                'u_bpc': bool(row['u_bpc'])
            }
            
            return summary
        
        return {}
```

## üéØ **PROCHAINE √âTAPE :**

**1. Cr√©er ces 3 fichiers dans Code Libraries**
**2. Finaliser votre notebook avec sauvegarde du mod√®le**
**3. Adapter les chemins de chargement dans `load_pretrained_model()`**

**Pr√™t pour l'impl√©mentation ?** üöÄL'utilisateur veut maintenant les codes des fichiers corrig√©s avec l'approche "mod√®le pr√©-entra√Æn√©" que j'ai recommand√©e. Je dois donc modifier les fichiers pour :

1. Retirer la m√©thode train_model() des Code Libraries
2. Ajouter une m√©thode load_pretrained_model() qui charge un mod√®le d√©j√† entra√Æn√©
3. Utiliser les vrais param√®tres optimaux bay√©siens qu'il m'a donn√©s
4. Corriger les erreurs que j'avais faites (feature_importances_ avec penalty='l1')
5. Simplifier l'architecture pour chargement rapide

Rappel des param√®tres optimaux de son mod√®le :
- C: 105.76211650904162
- penalty: 'l1'  
- solver: 'liblinear'
- max_iter: 2708
- class_weight: 'balanced'
- random_state: 42

Et les 5 features s√©lectionn√©es :
['dv_u_type_change_silca', 'dv_type', 'u_cab_count', 'u_bcr', 'u_bpc']

## üìÅ **FICHIERS CORRIG√âS - VERSION MOD√àLE PR√â-ENTRA√éN√â**

### **FICHIER 1: change_risk_predictor.py**

```python
"""
Module principal pour la pr√©diction des risques de changement
Mod√®le pr√©-entra√Æn√© pour chargement rapide
"""

import pandas as pd
import numpy as np
import joblib
from datetime import datetime

class ChangeRiskPredictor:
    """Pr√©dicteur de risque pour changements ServiceNow - Version pr√©-entra√Æn√©e"""
    
    def __init__(self):
        self.model = None
        self.selector = None
        self.feature_names = ['dv_u_type_change_silca', 'dv_type', 'u_cab_count', 'u_bcr', 'u_bpc']
        self.is_loaded = False
        
        # Chargement automatique au d√©marrage
        self.load_pretrained_model()
        
    def load_pretrained_model(self):
        """Charger le mod√®le et selector pr√©-entra√Æn√©s"""
        
        try:
            print("üîÑ Chargement du mod√®le pr√©-entra√Æn√©...")
            
            # TODO: Adapter le chemin selon votre configuration Dataiku
            # Option 1: Depuis managed folder
            # folder = dataiku.Folder("change_risk_models")
            # with folder.get_reader("model_final.pkl") as reader:
            #     self.model = joblib.load(reader)
            
            # Option 2: Depuis code libraries (temporaire)
            # self.model = joblib.load('model_final.pkl')
            # self.selector = joblib.load('selector_final.pkl')
            
            # Pour l'instant, on simule le chargement
            # Vous remplacerez par le vrai chargement apr√®s sauvegarde
            print("‚ö†Ô∏è Mod√®le pas encore sauvegard√© - simulation temporaire")
            self.is_loaded = False
            
        except Exception as e:
            print(f"‚ùå Erreur chargement mod√®le: {e}")
            print("üí° Assurez-vous que le mod√®le a √©t√© sauvegard√© dans le notebook final")
            self.is_loaded = False
    
    def predict_risk_score(self, change_data):
        """
        Pr√©dire le score de risque d'un changement
        Retourne un pourcentage (0-100%)
        """
        
        if not self.is_loaded:
            raise ValueError("‚ùå Mod√®le non charg√©. V√©rifiez la sauvegarde du mod√®le.")
        
        # Preprocessing
        change_features = self._prepare_single_change(change_data)
        change_selected = self.selector.transform(change_features)
        
        # Probabilit√© de probl√®me (classe 0)
        risk_probability = self.model.predict_proba(change_selected)[0, 0]
        
        # Convertir en pourcentage
        risk_score = risk_probability * 100
        
        return round(risk_score, 1)
    
    def get_detailed_analysis(self, change_data):
        """Analyse compl√®te d'un changement"""
        
        # Score de risque
        risk_score = self.predict_risk_score(change_data)
        
        # Niveau de risque
        risk_level = self._get_risk_level(risk_score)
        
        # Facteurs de risque d√©tect√©s
        risk_factors = self._analyze_risk_factors(change_data)
        
        # Recommandations
        recommendations = self._get_recommendations(risk_level['level'])
        
        return {
            'risk_score': risk_score,
            'risk_level': risk_level['level'],
            'risk_color': risk_level['color'],
            'interpretation': risk_level['interpretation'],
            'risk_factors': risk_factors,
            'recommendations': recommendations,
            'model_confidence': 'Mod√©r√©e (53% recall, 14% precision)',
            'model_info': 'LogisticRegression optimis√© bay√©sien'
        }
    
    def _get_risk_level(self, risk_score):
        """D√©terminer le niveau de risque"""
        
        if risk_score >= 75:
            return {
                'level': '√âLEV√â',
                'color': 'üî¥',
                'interpretation': 'Probabilit√© d\'√©chec importante'
            }
        elif risk_score >= 50:
            return {
                'level': 'MOYEN',
                'color': 'üü°', 
                'interpretation': 'Risque de complications possibles'
            }
        else:
            return {
                'level': 'FAIBLE',
                'color': 'üü¢',
                'interpretation': 'Profil de changement standard'
            }
    
    def _analyze_risk_factors(self, change_data):
        """Analyser les facteurs de risque bas√©s sur les 5 features r√©elles"""
        
        risk_factors = []
        
        # Extraire les valeurs des 5 features
        raw_values = {
            'dv_u_type_change_silca': change_data.get('dv_u_type_change_silca'),
            'dv_type': change_data.get('dv_type'),
            'u_cab_count': change_data.get('u_cab_count'),
            'u_bcr': change_data.get('u_bcr'),
            'u_bpc': change_data.get('u_bpc')
        }
        
        # Analyse bas√©e sur vos d√©couvertes d'exploration
        
        # 1. Type SILCA (votre top variable cat√©gorielle)
        if raw_values['dv_u_type_change_silca'] == 'Complex':
            risk_factors.append("Type de changement SILCA complexe")
        
        # 2. Type de changement
        if raw_values['dv_type'] in ['Urgent', 'Emergency']:
            risk_factors.append(f"Type de changement √† risque ({raw_values['dv_type']})")
        
        # 3. Nombre de CAB
        if raw_values['u_cab_count'] and raw_values['u_cab_count'] >= 3:
            risk_factors.append(f"Nombre √©lev√© de CAB requis ({raw_values['u_cab_count']})")
        
        # 4. P√©rim√®tre BCR
        if raw_values['u_bcr'] == True:
            risk_factors.append("P√©rim√®tre BCR impact√©")
        
        # 5. P√©rim√®tre BPC
        if raw_values['u_bpc'] == True:
            risk_factors.append("P√©rim√®tre BPC impact√©")
        
        return risk_factors
    
    def _get_recommendations(self, risk_level):
        """Recommandations selon le niveau de risque"""
        
        recommendations = {
            '√âLEV√â': [
                "R√©vision CAB recommand√©e",
                "Plan de rollback d√©taill√© requis", 
                "Tests approfondis conseill√©s",
                "Surveillance post-d√©ploiement renforc√©e"
            ],
            'MOYEN': [
                "Surveillance renforc√©e conseill√©e",
                "V√©rification des pr√©requis",
                "Communication √©quipe √©tendue",
                "Documentation des √©tapes critiques"
            ],
            'FAIBLE': [
                "Proc√©dure standard applicable",
                "Surveillance normale",
                "Documentation standard"
            ]
        }
        
        return recommendations.get(risk_level, [])
    
    def _prepare_single_change(self, change_data):
        """Pr√©parer les donn√©es d'un changement unique pour pr√©diction"""
        
        # Convertir en DataFrame si n√©cessaire
        if isinstance(change_data, dict):
            change_df = pd.DataFrame([change_data])
        else:
            change_df = change_data.copy()
        
        return change_df
    
    def get_model_info(self):
        """Informations sur le mod√®le charg√©"""
        
        if not self.is_loaded:
            return {"status": "Mod√®le non charg√©"}
        
        return {
            "status": "Mod√®le charg√©",
            "algorithm": "LogisticRegression",
            "optimization": "Bayesian (scikit-optimize)",
            "features": self.feature_names,
            "performance": {
                "recall": "53.1%",
                "precision": "14.2%", 
                "f1": "22.6%"
            },
            "hyperparameters": {
                "C": 105.76211650904162,
                "penalty": "l1",
                "solver": "liblinear",
                "max_iter": 2708
            }
        }
```

### **FICHIER 2: servicenow_connector.py**

```python
"""
Connecteur pour r√©cup√©rer les donn√©es ServiceNow et informations d'enrichissement
Version simplifi√©e pour mod√®le pr√©-entra√Æn√©
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re

class ServiceNowConnector:
    """Connecteur pour donn√©es ServiceNow et enrichissement"""
    
    def __init__(self):
        # Configuration des connexions (√† adapter selon votre environnement Dataiku)
        self.connection_status = "Simul√©"  # √Ä remplacer par vraie connexion
        
    def get_change_data(self, change_ref):
        """
        R√©cup√©rer les donn√©es d'un changement sp√©cifique
        
        Args:
            change_ref (str): R√©f√©rence du changement (ex: CHG001234)
            
        Returns:
            dict: Donn√©es du changement ou None si non trouv√©
        """
        
        # Validation du format
        if not self.validate_change_reference(change_ref):
            print(f"‚ùå Format de r√©f√©rence invalide: {change_ref}")
            return None
        
        try:
            # TODO: Remplacer par vraie requ√™te vers Snow Mirror
            # query = f"""
            # SELECT dv_u_type_change_silca, dv_type, u_cab_count, u_bcr, u_bpc,
            #        dv_assignment_group, dv_cmdb_ci, short_description, opened_at
            # FROM change_request 
            # WHERE number = '{change_ref}'
            # """
            
            # En attendant, simulation avec des donn√©es factices coh√©rentes
            change_data = {
                'number': change_ref,
                
                # === 5 FEATURES EXACTES DU MOD√àLE ===
                'dv_u_type_change_silca': np.random.choice(['Simple', 'Complex'], p=[0.7, 0.3]),
                'dv_type': np.random.choice(['Normal', 'Urgent', 'Emergency'], p=[0.8, 0.15, 0.05]),
                'u_cab_count': np.random.randint(0, 5),
                'u_bcr': np.random.choice([True, False], p=[0.3, 0.7]),
                'u_bpc': np.random.choice([True, False], p=[0.2, 0.8]),
                
                # === DONN√âES D'ENRICHISSEMENT ===
                'dv_assignment_group': np.random.choice([
                    '√âquipe Infrastructure', '√âquipe Application', 
                    '√âquipe R√©seau', '√âquipe S√©curit√©'
                ]),
                'dv_cmdb_ci': f'Server-{np.random.choice(["PROD", "PREP", "DEV"])}-{np.random.randint(100, 999):03d}',
                'dv_category': np.random.choice(['Infrastructure', 'Application', 'R√©seau']),
                'opened_at': datetime.now() - timedelta(days=np.random.randint(0, 3)),
                'short_description': f'Changement de maintenance {change_ref}',
                'dv_impact': np.random.choice(['Faible', 'Moyen', '√âlev√©'], p=[0.6, 0.3, 0.1])
            }
            
            print(f"‚úÖ Changement {change_ref} r√©cup√©r√©")
            return change_data
            
        except Exception as e:
            print(f"‚ùå Erreur r√©cup√©ration changement {change_ref}: {e}")
            return None
    
    def get_team_statistics(self, assignment_group, months_back=6):
        """
        R√©cup√©rer les statistiques d'une √©quipe
        
        Args:
            assignment_group (str): Nom de l'√©quipe
            months_back (int): Nombre de mois √† analyser
        """
        
        try:
            # TODO: Vraie requ√™te SQL vers Snow Mirror
            # query = f"""
            # SELECT COUNT(*) as total,
            #        SUM(CASE WHEN dv_close_code = 'Succ√®s' THEN 1 ELSE 0 END) as successes
            # FROM change_request 
            # WHERE dv_assignment_group = '{assignment_group}' 
            # AND opened_at >= DATE_SUB(NOW(), INTERVAL {months_back} MONTH)
            # """
            
            # Simulation bas√©e sur des patterns r√©alistes
            total_changes = np.random.randint(20, 150)
            
            # Certaines √©quipes plus fiables que d'autres
            if 'Infrastructure' in assignment_group:
                success_rate = np.random.uniform(0.85, 0.95)
            elif 'Application' in assignment_group:
                success_rate = np.random.uniform(0.75, 0.90)
            else:
                success_rate = np.random.uniform(0.70, 0.85)
            
            successes = int(total_changes * success_rate)
            failures = total_changes - successes
            
            team_stats = {
                'assignment_group': assignment_group,
                'period_months': months_back,
                'total_changes': total_changes,
                'successes': successes,
                'failures': failures,
                'success_rate': round(success_rate * 100, 1),
                'last_failure_date': datetime.now() - timedelta(days=np.random.randint(1, 30)) if failures > 0 else None,
                'trend': 'stable'  # TODO: Calculer vraie tendance
            }
            
            print(f"‚úÖ Statistiques √©quipe {assignment_group} r√©cup√©r√©es")
            return team_stats
            
        except Exception as e:
            print(f"‚ùå Erreur stats √©quipe {assignment_group}: {e}")
            return None
    
    def get_solution_incidents(self, cmdb_ci, months_back=3):
        """
        R√©cup√©rer les incidents li√©s √† une solution/CI
        """
        
        try:
            # TODO: Requ√™te vers table incidents ServiceNow
            
            # Simulation r√©aliste
            incident_count = np.random.poisson(2)  # Distribution de Poisson
            
            incidents_data = {
                'cmdb_ci': cmdb_ci,
                'period_months': months_back,
                'total_incidents': incident_count,
                'critical_incidents': max(0, incident_count - np.random.randint(0, 2)),
                'avg_resolution_hours': np.random.randint(1, 24) if incident_count > 0 else 0,
                'last_incident_date': datetime.now() - timedelta(days=np.random.randint(1, 90)) if incident_count > 0 else None,
                'incident_types': ['Performance', 'Disponibilit√©', 'S√©curit√©'][:incident_count]
            }
            
            print(f"‚úÖ Incidents {cmdb_ci} r√©cup√©r√©s")
            return incidents_data
            
        except Exception as e:
            print(f"‚ùå Erreur incidents {cmdb_ci}: {e}")
            return None
    
    def find_similar_changes(self, change_data, limit=10):
        """
        Trouver les changements similaires bas√©s sur r√®gles m√©tier
        Bas√© sur vos d√©couvertes d'analyse exploratoire
        """
        
        try:
            # Crit√®res de similarit√© (bas√©s sur vos variables importantes)
            base_criteria = {
                'dv_u_type_change_silca': change_data.get('dv_u_type_change_silca'),
                'dv_type': change_data.get('dv_type'),
                'dv_assignment_group': change_data.get('dv_assignment_group'),
                'dv_category': change_data.get('dv_category')
            }
            
            # TODO: Vraie requ√™te SQL avec scoring de similarit√©
            
            # Simulation de changements similaires avec distribution r√©aliste
            similar_changes = []
            
            for i in range(min(limit, np.random.randint(3, 8))):
                
                # Simuler r√©sultats avec distribution proche de la r√©alit√© (90% succ√®s)
                close_code = np.random.choice([
                    'Succ√®s', 'Succ√®s avec difficult√©s', 'Impl√©ment√© partiellement',
                    '√âchec avec retour arri√®re', '√âchec sans retour arri√®re'
                ], p=[0.896, 0.054, 0.021, 0.016, 0.013])  # Vos vraies distributions
                
                similar_change = {
                    'number': f'CHG{np.random.randint(1000000, 9999999):07d}',
                    'dv_close_code': close_code,
                    'short_description': f'Changement similaire - {base_criteria["dv_type"]} sur {base_criteria["dv_u_type_change_silca"]}',
                    'opened_at': datetime.now() - timedelta(days=np.random.randint(30, 365)),
                    'similarity_score': np.random.randint(60, 95),
                    'close_notes': self._generate_close_notes(close_code),
                    'assignment_group': change_data.get('dv_assignment_group'),
                    'duration_hours': np.random.randint(1, 8)
                }
                
                similar_changes.append(similar_change)
            
            # Trier par score de similarit√©
            similar_changes.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            print(f"‚úÖ {len(similar_changes)} changements similaires trouv√©s")
            return similar_changes
            
        except Exception as e:
            print(f"‚ùå Erreur changements similaires: {e}")
            return []
    
    def _generate_close_notes(self, close_code):
        """G√©n√©rer des notes de fermeture r√©alistes"""
        
        notes_templates = {
            'Succ√®s': [
                'Changement appliqu√© avec succ√®s. Aucun incident d√©tect√©.',
                'D√©ploiement r√©alis√© conform√©ment au planning. Tests OK.',
                'Migration termin√©e sans probl√®me. Services op√©rationnels.'
            ],
            'Succ√®s avec difficult√©s': [
                'Changement r√©alis√© mais quelques difficult√©s mineures rencontr√©es.',
                'D√©ploiement ok apr√®s r√©solution probl√®me configuration.',
                'Succ√®s final apr√®s ajustement proc√©dure.'
            ],
            '√âchec avec retour arri√®re': [
                '√âchec d√©tect√©, rollback effectu√© avec succ√®s.',
                'Probl√®me critique, retour √† l\'√©tat ant√©rieur.',
                'Changement annul√©, syst√®me restaur√©.'
            ],
            '√âchec sans retour arri√®re': [
                '√âchec partiel, correction manuelle appliqu√©e.',
                'Probl√®me r√©solu par intervention d\'urgence.',
                'Echec mineur, service maintenu.'
            ],
            'Impl√©ment√© partiellement': [
                'Changement partiellement r√©alis√© selon planning.',
                'Phase 1 termin√©e, phase 2 report√©e.',
                'Impl√©mentation partielle conforme aux objectifs.'
            ]
        }
        
        templates = notes_templates.get(close_code, ['Notes de fermeture standard.'])
        return np.random.choice(templates)
    
    def validate_change_reference(self, change_ref):
        """Valider le format de la r√©f√©rence changement ServiceNow"""
        
        # Format ServiceNow standard: CHG + 7 chiffres
        pattern = r'^CHG\d{7}$'
        
        if re.match(pattern, change_ref):
            return True
        else:
            return False
    
    def get_connection_status(self):
        """V√©rifier le statut de connexion Snow Mirror"""
        
        return {
            'status': self.connection_status,
            'last_sync': datetime.now() - timedelta(hours=1),
            'available_tables': ['change_request', 'incident', 'cmdb_ci']
        }
```

### **FICHIER 3: data_preprocessing.py**

```python
"""
Module pour le preprocessing des donn√©es de changement
Version simplifi√©e pour mod√®le pr√©-entra√Æn√©
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

class ChangeDataPreprocessor:
    """Preprocessing des donn√©es de changement - Version pr√©-entra√Æn√©e"""
    
    def __init__(self):
        # Les encoders seront charg√©s avec le mod√®le pr√©-entra√Æn√©
        self.label_encoders = {}
        self.is_fitted = False
        
    def load_fitted_preprocessor(self, encoders_dict):
        """Charger les encoders pr√©-entra√Æn√©s"""
        
        self.label_encoders = encoders_dict
        self.is_fitted = True
        print("‚úÖ Preprocessor pr√©-entra√Æn√© charg√©")
        
    def transform_single_change(self, change_data):
        """
        Transformer un changement unique (pour pr√©diction en temps r√©el)
        Optimis√© pour l'utilisation dans la webapp
        """
        
        # Convertir en DataFrame
        if isinstance(change_data, dict):
            df = pd.DataFrame([change_data])
        else:
            df = change_data.copy()
        
        # 1. Garder seulement les 5 features du mod√®le
        required_features = ['dv_u_type_change_silca', 'dv_type', 'u_cab_count', 'u_bcr', 'u_bpc']
        
        # V√©rifier que toutes les features sont pr√©sentes
        for feature in required_features:
            if feature not in df.columns:
                print(f"‚ö†Ô∏è Feature manquante: {feature}")
                df[feature] = 0  # Valeur par d√©faut
        
        # Garder seulement les features n√©cessaires
        df_features = df[required_features].copy()
        
        # 2. Encodage des variables cat√©gorielles
        df_encoded = self._encode_categorical_single(df_features)
        
        # 3. Imputation des valeurs manquantes
        df_final = self._imputation_single(df_encoded)
        
        return df_final
    
    def _encode_categorical_single(self, df):
        """Encoder les variables cat√©gorielles pour un changement unique"""
        
        df_encoded = df.copy()
        
        # Mapping des valeurs bas√© sur vos donn√©es d'entra√Ænement
        # TODO: Ces mappings doivent √™tre sauvegard√©s avec le mod√®le pr√©-entra√Æn√©
        
        default_encodings = {
            'dv_u_type_change_silca': {'Simple': 0, 'Complex': 1},
            'dv_type': {'Normal': 0, 'Urgent': 1, 'Emergency': 2}
        }
        
        for col in ['dv_u_type_change_silca', 'dv_type']:
            if col in df_encoded.columns:
                if col in default_encodings:
                    # Utiliser le mapping par d√©faut
                    mapping = default_encodings[col]
                    df_encoded[col] = df_encoded[col].map(mapping).fillna(0)
                else:
                    # Encoder num√©riquement si nouvelle valeur
                    df_encoded[col] = pd.Categorical(df_encoded[col]).codes
        
        return df_encoded
    
    def _imputation_single(self, df):
        """Imputation des valeurs manquantes pour un changement unique"""
        
        df_imputed = df.copy()
        
        # Imputation avec valeurs par d√©faut bas√©es sur vos analyses
        default_values = {
            'dv_u_type_change_silca': 0,  # Simple (valeur la plus fr√©quente)
            'dv_type': 0,                 # Normal (valeur la plus fr√©quente)
            'u_cab_count': 1,             # Valeur m√©diane typique
            'u_bcr': 0,                   # False encod√© (plus fr√©quent)
            'u_bpc': 0                    # False encod√© (plus fr√©quent)
        }
        
        for col, default_val in default_values.items():
            if col in df_imputed.columns:
                df_imputed[col].fillna(default_val, inplace=True)
        
        # S'assurer que tout est num√©rique
        for col in df_imputed.columns:
            df_imputed[col] = pd.to_numeric(df_imputed[col], errors='coerce').fillna(0)
        
        return df_imputed
    
    def validate_input_data(self, change_data):
        """Valider les donn√©es d'entr√©e"""
        
        required_features = ['dv_u_type_change_silca', 'dv_type', 'u_cab_count', 'u_bcr', 'u_bpc']
        
        validation_results = {
            'is_valid': True,
            'missing_features': [],
            'invalid_values': [],
            'warnings': []
        }
        
        # V√©rifier pr√©sence des features
        for feature in required_features:
            if feature not in change_data:
                validation_results['missing_features'].append(feature)
                validation_results['warnings'].append(f"Feature {feature} manquante, valeur par d√©faut utilis√©e")
        
        # V√©rifier types et valeurs
        if 'u_cab_count' in change_data:
            if not isinstance(change_data['u_cab_count'], (int, float)) or change_data['u_cab_count'] < 0:
                validation_results['invalid_values'].append('u_cab_count doit √™tre un nombre positif')
        
        if 'dv_u_type_change_silca' in change_data:
            if change_data['dv_u_type_change_silca'] not in ['Simple', 'Complex']:
                validation_results['warnings'].append('Type SILCA inconnu, trait√© comme Simple')
        
        if 'dv_type' in change_data:
            if change_data['dv_type'] not in ['Normal', 'Urgent', 'Emergency']:
                validation_results['warnings'].append('Type de changement inconnu, trait√© comme Normal')
        
        return validation_results
    
    def get_feature_summary(self, processed_data):
        """R√©sum√© des features apr√®s preprocessing"""
        
        if isinstance(processed_data, pd.DataFrame) and len(processed_data) > 0:
            row = processed_data.iloc[0]
            
            # D√©coder les valeurs pour affichage
            silca_types = {0: 'Simple', 1: 'Complex'}
            change_types = {0: 'Normal', 1: 'Urgent', 2: 'Emergency'}
            
            summary = {
                'dv_u_type_change_silca': silca_types.get(row['dv_u_type_change_silca'], 'Inconnu'),
                'dv_type': change_types.get(row['dv_type'], 'Inconnu'),
                'u_cab_count': int(row['u_cab_count']),
                'u_bcr': bool(row['u_bcr']),
                'u_bpc': bool(row['u_bpc'])
            }
            
            return summary
        
        return {}
```

## üéØ **PROCHAINE √âTAPE :**

**1. Cr√©er ces 3 fichiers dans Code Libraries**
**2. Finaliser votre notebook avec sauvegarde du mod√®le**
**3. Adapter les chemins de chargement dans `load_pretrained_model()`**

**Pr√™t pour l'impl√©mentation ?** üöÄ
