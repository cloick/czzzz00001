Excellent ! F√©licitations pour avoir obtenu la licence Dataiku ! Je vais vous guider pas √† pas pour impl√©menter votre projet de classification des tickets METIS sur Dataiku.

## üöÄ Guide complet Dataiku pour votre projet

### 1. Premiers pas avec Dataiku

**Configuration initiale :**
- Connectez-vous √† votre instance Dataiku
- Cr√©ez un nouveau projet : "Classification_Tickets_METIS"
- D√©finissez les permissions et partagez avec votre √©quipe

### 2. Import et pr√©paration des donn√©es

**√âtape 1 : Importer vos datasets**
```
1. Cliquez sur "IMPORT DATASET"
2. Choisissez votre source :
   - Fichiers CSV : metis_tickets.csv et gdp_tickets.csv
   - Ou connexion directe √† votre base de donn√©es
3. Pr√©visualisez et validez l'import
```

**√âtape 2 : Pr√©parer les donn√©es**
- Utilisez un recipe "Prepare" pour :
  - Nettoyer le texte des notes de r√©solution
  - G√©rer les valeurs manquantes
  - Cr√©er la colonne 'est_fiable'
  - Encoder les variables cat√©gorielles

### 3. Cr√©ation des embeddings avec CamemBERT

**Dans Dataiku :**
```python
# Recipe Python pour g√©n√©rer les embeddings
import dataiku
import pandas as pd
from transformers import CamembertTokenizer, CamembertModel
import torch

# Lire le dataset
df = dataiku.Dataset("metis_tickets_prepared").get_dataframe()

# Charger CamemBERT
tokenizer = CamembertTokenizer.from_pretrained('camembert-base')
model = CamembertModel.from_pretrained('camembert-base')

# Fonction pour g√©n√©rer les embeddings
def get_embeddings(texts, batch_size=16):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        inputs = tokenizer(batch, padding=True, truncation=True, 
                          max_length=128, return_tensors='pt')
        with torch.no_grad():
            outputs = model(**inputs)
            # Utiliser le token [CLS]
            batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()
        embeddings.extend(batch_embeddings)
    return embeddings

# Appliquer sur vos donn√©es
df['embeddings'] = get_embeddings(df['notes_resolution_nettoyees'].tolist())
```

### 4. Impl√©mentation du clustering HDBSCAN

**Cr√©er un recipe Python pour le clustering :**
```python
# Enrichir les embeddings avec les variables cat√©gorielles
from sklearn.preprocessing import StandardScaler
import numpy as np

# Pr√©parer les features cat√©gorielles
cat_features = ['Groupe_affect√©_encoded', 'Service_m√©tier_encoded', 
                'Cat1_encoded', 'Cat2_encoded', 'Priorit√©_encoded']

# Normaliser et pond√©rer
scaler = StandardScaler()
cat_data = scaler.fit_transform(df[cat_features])

# Pond√©ration comme dans votre approche
weights = {'Groupe_affect√©': 3.0, 'Service_m√©tier': 2.0, 
           'Cat1': 1.0, 'Cat2': 1.0, 'Priorit√©': 0.5}

# Combiner embeddings et features cat√©gorielles
embeddings_enriched = np.hstack([df['embeddings'].tolist(), cat_data])

# UMAP + HDBSCAN avec vos param√®tres optimaux
from umap import UMAP
import hdbscan

reducer = UMAP(n_components=2, random_state=42, n_neighbors=15)
embeddings_2d = reducer.fit_transform(embeddings_enriched)

clusterer = hdbscan.HDBSCAN(
    min_cluster_size=240,
    min_samples=20,
    cluster_selection_epsilon=1.56,
    metric='euclidean'
)
df['cluster'] = clusterer.fit_predict(embeddings_2d)
```

### 5. Configuration du mod√®le LLM dans Dataiku

**Utiliser les LLM Labs de Dataiku :**
1. Allez dans "Lab" > "LLM Experimentation"
2. Configurez votre mod√®le (CamemBERT ou mDeBERTa)
3. Cr√©ez un prompt pour la classification des tickets

### 6. Cr√©ation du classificateur hybride

**Recipe Python pour le vote pond√©r√© :**
```python
def classify_hybrid(row, llm_predictions, cluster_to_cause):
    # R√©cup√©rer les pr√©dictions
    llm_pred = llm_predictions.get(row['N¬∞ INC'])
    cluster_pred = cluster_to_cause.get(row['cluster'], '√Ä d√©terminer')
    
    # Syst√®me de vote pond√©r√©
    weights = {
        'llm': 0.6,
        'clustering': 0.4
    }
    
    # Logique de combinaison
    if cluster_pred == '√Ä d√©terminer':
        return llm_pred
    elif llm_pred == cluster_pred:
        return llm_pred
    else:
        # Vote pond√©r√© si d√©saccord
        # Impl√©menter votre logique sp√©cifique
        return llm_pred  # ou cluster_pred selon confiance
```

### 7. Cr√©ation du pipeline de d√©ploiement

**Dans Dataiku Flow :**
1. Cr√©ez un scenario pour automatiser :
   - Import des nouvelles donn√©es
   - Pr√©paration
   - G√©n√©ration d'embeddings
   - Clustering
   - Classification LLM
   - Vote hybride
   - Export des r√©sultats

2. Configurez les triggers (temps r√©el, batch, etc.)

### 8. Monitoring et visualisations

**Cr√©er un dashboard Dataiku :**
- Distribution des clusters
- Score de silhouette en temps r√©el
- M√©triques de performance
- √âvolution du volume de tickets

### 9. API pour le d√©ploiement

```python
# Endpoint API dans Dataiku
def predict_ticket_cause(ticket_data):
    # Pr√©traitement
    # Embedding
    # Pr√©diction hybride
    return {
        'cause': predicted_cause,
        'confidence': confidence_score,
        'method': 'hybrid'
    }
```

### üí° Conseils pratiques :

1. **Commencez petit** : Testez d'abord sur un √©chantillon de 1000 tickets
2. **Utilisez les Visual Recipes** : Dataiku offre des interfaces visuelles pour beaucoup d'op√©rations
3. **Versioning** : Utilisez le versioning de Dataiku pour tracker vos mod√®les
4. **Collaboration** : Partagez votre projet avec l'√©quipe pour review

Avez-vous des questions sp√©cifiques sur l'une de ces √©tapes ? Par quoi souhaitez-vous commencer ?
