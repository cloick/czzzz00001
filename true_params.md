# -*- coding: utf-8 -*-
import dataiku
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from umap import UMAP
import hdbscan
from sklearn.metrics import silhouette_score

print("ğŸ¯ CLUSTERING AVEC PARAMÃˆTRES OPTIMAUX (PC LOCAL)")
print("="*60)

# Read recipe inputs
dataset = dataiku.Dataset("incident_prepared_embeddings_creation")
df = dataset.get_dataframe()

print(f"Dataset chargÃ© : {len(df)} lignes, {len(df.columns)} colonnes")

# RÃ©cupÃ©rer les colonnes d'embeddings
embedding_cols = [col for col in df.columns if col.startswith('embedding_')]
print(f"Colonnes d'embeddings trouvÃ©es : {len(embedding_cols)}")

if len(embedding_cols) == 0:
    raise ValueError("Aucune colonne d'embedding trouvÃ©e!")

embeddings = df[embedding_cols].values
print(f"Shape des embeddings : {embeddings.shape}")

# RÃ©cupÃ©rer et encoder les variables catÃ©gorielles
cat_vars = ['PrioritÃ©', 'Service mÃ©tier', 'Cat1', 'Cat2', 'Groupe affectÃ©']
cat_features = []

print("\nğŸ”§ Encodage des variables catÃ©gorielles...")
for col in cat_vars:
    if col in df.columns:
        # Encodage simple par valeurs uniques
        unique_vals = df[col].fillna('INCONNU').unique()
        mapping = {val: i for i, val in enumerate(unique_vals)}
        encoded = df[col].fillna('INCONNU').map(mapping)
        cat_features.append(encoded.values)
        print(f"  âœ… {col}: {len(unique_vals)} valeurs uniques")
    else:
        print(f"  âš ï¸  {col} non trouvÃ© dans le dataset")

# Combiner embeddings et features catÃ©gorielles
if cat_features:
    cat_features_array = np.array(cat_features).T
    
    # Normaliser les features catÃ©gorielles
    scaler = StandardScaler()
    cat_features_scaled = scaler.fit_transform(cat_features_array)
    
    # PondÃ©ration des features (comme dans votre code PC)
    weights = np.array([0.5, 2.0, 1.0, 1.0, 3.0])[:len(cat_features)]
    cat_features_weighted = cat_features_scaled * weights
    
    # Combiner
    features_combined = np.hstack([embeddings, cat_features_weighted])
    print(f"âœ… Features combinÃ©es : {features_combined.shape}")
else:
    features_combined = embeddings
    print("âš ï¸  Utilisation des embeddings seuls")

# UMAP pour rÃ©duction dimensionnelle (paramÃ¨tres identiques Ã  votre PC)
print("\nğŸ—ºï¸  RÃ©duction dimensionnelle avec UMAP...")
reducer_2d = UMAP(
    n_components=2, 
    random_state=42, 
    n_neighbors=15, 
    min_dist=0.1
)
embeddings_2d = reducer_2d.fit_transform(features_combined)

reducer_3d = UMAP(
    n_components=3, 
    random_state=42, 
    n_neighbors=15, 
    min_dist=0.1
)
embeddings_3d = reducer_3d.fit_transform(features_combined)

print(f"âœ… UMAP 2D : {embeddings_2d.shape}")
print(f"âœ… UMAP 3D : {embeddings_3d.shape}")

# HDBSCAN avec VOS PARAMÃˆTRES OPTIMAUX exactement
print("\nğŸ¯ Clustering HDBSCAN avec PARAMÃˆTRES OPTIMAUX...")
print("   (paramÃ¨tres qui fonctionnaient sur votre PC)")

clusterer = hdbscan.HDBSCAN(
    min_cluster_size=240,        # VOS paramÃ¨tres optimaux
    min_samples=20,
    cluster_selection_epsilon=1.56,
    metric='euclidean'
)

print("ğŸ”„ Application du clustering...")
cluster_labels = clusterer.fit_predict(embeddings_2d)

# Statistiques du clustering
n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
n_noise = (cluster_labels == -1).sum()
noise_ratio = n_noise / len(cluster_labels)

print(f"\nğŸ“Š RÃ‰SULTATS DU CLUSTERING:")
print(f"  ğŸ¯ Clusters dÃ©tectÃ©s : {n_clusters}")
print(f"  ğŸ”‡ Points de bruit : {n_noise} ({noise_ratio:.2%})")

# Score de silhouette
silhouette_score_val = None
if n_clusters > 1:
    mask = cluster_labels != -1
    if mask.sum() > n_clusters:
        try:
            silhouette_score_val = silhouette_score(embeddings_2d[mask], cluster_labels[mask])
            print(f"  ğŸ“ˆ Score de silhouette : {silhouette_score_val:.4f}")
        except Exception as e:
            print(f"  âš ï¸  Score de silhouette : Non calculable ({e})")

# Distribution des clusters
print(f"\nğŸ“Š DISTRIBUTION DES CLUSTERS:")
cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()

for cluster, count in cluster_counts.items():
    if cluster == -1:
        print(f"  Bruit (cluster -1) : {count:4d} tickets ({count/len(cluster_labels)*100:.1f}%)")
    else:
        print(f"  Cluster {cluster:2d} : {count:4d} tickets ({count/len(cluster_labels)*100:.1f}%)")

# Ajouter les rÃ©sultats au dataframe
df['cluster'] = cluster_labels
df['umap_x'] = embeddings_2d[:, 0]
df['umap_y'] = embeddings_2d[:, 1]
df['umap_z'] = embeddings_3d[:, 0]  # PremiÃ¨re dimension 3D

# Analyse rapide des clusters par tickets fiables
if 'est_fiable' in df.columns:
    print(f"\nğŸ” ANALYSE RAPIDE (tickets fiables):")
    
    for cluster in sorted(set(cluster_labels)):
        if cluster == -1:
            continue
            
        cluster_data = df[df['cluster'] == cluster]
        fiables = cluster_data[cluster_data['est_fiable']]
        
        if len(fiables) > 0 and 'cause' in fiables.columns:
            causes = fiables['cause'].value_counts()
            if len(causes) > 0:
                cause_principale = causes.index[0]
                count_principale = causes.iloc[0]
                confidence = count_principale / len(fiables)
                print(f"  Cluster {cluster:2d}: {len(cluster_data):4d} tickets, "
                      f"{len(fiables):2d} fiables â†’ {cause_principale} (conf: {confidence:.2f})")
            else:
                print(f"  Cluster {cluster:2d}: {len(cluster_data):4d} tickets â†’ Ã€ dÃ©terminer")
        else:
            print(f"  Cluster {cluster:2d}: {len(cluster_data):4d} tickets â†’ Ã€ dÃ©terminer")

# Comparaison avec l'objectif
print(f"\nğŸ¯ Ã‰VALUATION PAR RAPPORT Ã€ L'OBJECTIF:")
target_clusters = 15
if n_clusters <= target_clusters + 5:  # TolÃ©rance de 5
    print(f"  âœ… EXCELLENT! {n_clusters} clusters dÃ©tectÃ©s (objectif: ~{target_clusters})")
elif n_clusters <= target_clusters + 10:
    print(f"  ğŸŸ¡ BON: {n_clusters} clusters dÃ©tectÃ©s (objectif: ~{target_clusters})")
else:
    print(f"  âŒ TROP: {n_clusters} clusters dÃ©tectÃ©s (objectif: ~{target_clusters})")

if noise_ratio < 0.1:
    print(f"  âœ… Taux de bruit excellent: {noise_ratio:.2%}")
elif noise_ratio < 0.2:
    print(f"  ğŸŸ¡ Taux de bruit acceptable: {noise_ratio:.2%}")
else:
    print(f"  âš ï¸  Taux de bruit Ã©levÃ©: {noise_ratio:.2%}")

if silhouette_score_val and silhouette_score_val > 0.5:
    print(f"  âœ… Score de silhouette excellent: {silhouette_score_val:.4f}")
elif silhouette_score_val and silhouette_score_val > 0.3:
    print(f"  ğŸŸ¡ Score de silhouette correct: {silhouette_score_val:.4f}")
elif silhouette_score_val:
    print(f"  âš ï¸  Score de silhouette faible: {silhouette_score_val:.4f}")

# Sauvegarder le rÃ©sultat
print(f"\nğŸ’¾ SAUVEGARDE...")
output = dataiku.Dataset("incident_with_clusters_optimal")
output.write_with_schema(df)

print(f"âœ… Clustering terminÃ© avec succÃ¨s!")
print(f"ğŸ“ Dataset de sortie : incident_with_clusters_optimal ({len(df)} lignes)")
print("="*60)
print("ğŸ‰ PRÃŠT pour l'analyse des causes et la suite du pipeline!")
