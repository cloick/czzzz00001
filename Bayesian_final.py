# -*- coding: utf-8 -*-
import dataiku
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import hdbscan
from sklearn.metrics import silhouette_score
import warnings

# Supprimer TOUS les warnings
warnings.filterwarnings("ignore")

print("ğŸš€ CLUSTERING AVEC OPTIMISATION INTENSIVE - 100 Ã‰VALUATIONS")
print("ğŸ¯ Recherche exhaustive pour le MEILLEUR rÃ©sultat possible")
print("="*80)

# Read recipe inputs
dataset = dataiku.Dataset("incident_prepared_embeddings_creation")
df = dataset.get_dataframe()

print(f"Dataset chargÃ© : {len(df)} lignes, {len(df.columns)} colonnes")

# RÃ©cupÃ©rer les colonnes d'embeddings
embedding_cols = [col for col in df.columns if col.startswith('embedding_')]
print(f"Colonnes d'embeddings trouvÃ©es : {len(embedding_cols)}")

embeddings = df[embedding_cols].values
print(f"Shape des embeddings : {embeddings.shape}")

# Variables catÃ©gorielles avec encodage robuste
cat_vars = ['PrioritÃ©', 'Service mÃ©tier', 'Cat1', 'Cat2', 'Groupe affectÃ©']
cat_features = []

print("\nğŸ”§ Encodage des variables catÃ©gorielles...")
for col in cat_vars:
    if col in df.columns:
        unique_vals = df[col].fillna('INCONNU').unique()
        mapping = {val: i for i, val in enumerate(unique_vals)}
        encoded = df[col].fillna('INCONNU').map(mapping)
        cat_features.append(encoded.values)
        print(f"  âœ… {col}: {len(unique_vals)} valeurs uniques")

# Combiner features avec pondÃ©ration optimisÃ©e
if cat_features:
    cat_features_array = np.array(cat_features).T
    scaler = StandardScaler()
    cat_features_scaled = scaler.fit_transform(cat_features_array)
    
    # PondÃ©ration renforcÃ©e
    weights = np.array([0.8, 3.0, 1.5, 1.5, 4.0])[:len(cat_features)]  # Plus de poids
    cat_features_weighted = cat_features_scaled * weights
    features_combined = np.hstack([embeddings, cat_features_weighted])
    print(f"âœ… Features combinÃ©es avec pondÃ©ration renforcÃ©e : {features_combined.shape}")
else:
    features_combined = embeddings

# UMAP optimisÃ© et robuste
print("\nğŸ—ºï¸  RÃ©duction dimensionnelle UMAP optimisÃ©e...")

try:
    from umap import UMAP
    
    # Configuration haute performance
    import numba
    numba.config.THREADING_LAYER = 'workqueue'
    
    reducer_2d = UMAP(
        n_components=2,
        n_neighbors=min(50, len(features_combined) - 1),  # Plus de voisins
        min_dist=0.05,      # Plus prÃ©cis
        metric='cosine',
        random_state=42,
        init='spectral',    # Meilleure initialisation
        verbose=False,
        low_memory=False,   # Utiliser toute la mÃ©moire disponible
        n_epochs=500        # Plus d'Ã©poques pour convergence
    )
    
    print("ğŸ”„ UMAP 2D haute qualitÃ© en cours...")
    embeddings_2d = reducer_2d.fit_transform(features_combined)
    
    reducer_3d = UMAP(
        n_components=3,
        n_neighbors=min(50, len(features_combined) - 1),
        min_dist=0.05,
        metric='cosine',
        random_state=42,
        init='spectral',
        verbose=False,
        low_memory=False,
        n_epochs=500
    )
    
    print("ğŸ”„ UMAP 3D haute qualitÃ© en cours...")
    embeddings_3d = reducer_3d.fit_transform(features_combined)
    
    print(f"âœ… UMAP 2D haute qualitÃ© : {embeddings_2d.shape}")
    print(f"âœ… UMAP 3D haute qualitÃ© : {embeddings_3d.shape}")
    
except Exception as e:
    print(f"âš ï¸  UMAP Ã©chouÃ©, fallback PCA optimisÃ© : {e}")
    from sklearn.decomposition import PCA
    
    # PCA avec plus de composantes intermÃ©diaires
    pca_2d = PCA(n_components=2, random_state=42, whiten=True)
    embeddings_2d = pca_2d.fit_transform(features_combined)
    
    pca_3d = PCA(n_components=3, random_state=42, whiten=True)
    embeddings_3d = pca_3d.fit_transform(features_combined)

# OPTIMISATION INTENSIVE - 100 Ã‰VALUATIONS
print(f"\nğŸ”¬ OPTIMISATION BAYÃ‰SIENNE INTENSIVE")
print("="*70)

try:
    from skopt import gp_minimize
    from skopt.space import Real, Integer, Categorical
    from skopt.utils import use_named_args
    from skopt import gp_minimize
    
    # ESPACE DE RECHERCHE Ã‰LARGI
    space = [
        Integer(30, 1000, name='min_cluster_size'),        # TrÃ¨s large gamme
        Integer(3, 200, name='min_samples'),               # TrÃ¨s large gamme  
        Real(0.05, 4.0, name='cluster_selection_epsilon'), # Gamme Ã©tendue
        Categorical(['euclidean', 'manhattan', 'cosine'], name='metric'),  # Test diffÃ©rentes mÃ©triques
        Categorical(['eom', 'leaf'], name='cluster_selection_method')      # MÃ©thodes de sÃ©lection
    ]
    
    # Objectifs multiples
    target_ranges = [
        {'min': 10, 'max': 25, 'weight': 1.0, 'name': '10-25 clusters'},
        {'min': 25, 'max': 50, 'weight': 0.8, 'name': '25-50 clusters'},  
        {'min': 50, 'max': 80, 'weight': 0.6, 'name': '50-80 clusters'}
    ]
    
    print(f"ğŸ¯ ESPACE DE RECHERCHE Ã‰LARGI :")
    print(f"   min_cluster_size : [30, 1000]")
    print(f"   min_samples : [3, 200]")
    print(f"   cluster_selection_epsilon : [0.05, 4.0]")
    print(f"   metric : ['euclidean', 'manhattan', 'cosine']")
    print(f"   cluster_selection_method : ['eom', 'leaf']")
    print(f"ğŸ¯ OBJECTIFS MULTIPLES testÃ©s en parallÃ¨le")
    
    # Variables pour tracker les meilleurs rÃ©sultats
    best_results = []
    evaluation_count = 0
    
    @use_named_args(space)
    def objective_intensive(min_cluster_size, min_samples, cluster_selection_epsilon, metric, cluster_selection_method):
        global evaluation_count, best_results
        evaluation_count += 1
        
        # Validation des paramÃ¨tres
        if min_samples > min_cluster_size:
            return 100.0
        
        try:
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=int(min_cluster_size),
                min_samples=int(min_samples),
                cluster_selection_epsilon=float(cluster_selection_epsilon),
                metric=metric,
                cluster_selection_method=cluster_selection_method
            )
            
            labels = clusterer.fit_predict(embeddings_2d)
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            noise_ratio = (labels == -1).sum() / len(labels)
            
            # Calculer silhouette
            silhouette = -2.0  # Valeur par dÃ©faut trÃ¨s basse
            if n_clusters > 1:
                mask = labels != -1
                if mask.sum() > n_clusters and mask.sum() < 20000:  # Limite performance
                    try:
                        silhouette = silhouette_score(embeddings_2d[mask], labels[mask])
                    except:
                        silhouette = -2.0
            
            # Fonction objectif sophistiquÃ©e multi-objectifs
            best_objective = 200.0
            best_target = None
            
            for target in target_ranges:
                if target['min'] <= n_clusters <= target['max']:
                    # Dans une plage cible
                    cluster_penalty = 0  # Pas de pÃ©nalitÃ©
                    
                    # Bonus pour silhouette positive
                    silhouette_bonus = max(0, silhouette * 5.0)
                    
                    # PÃ©nalitÃ© pour bruit excessif
                    noise_penalty = max(0, (noise_ratio - 0.1) * 10.0)
                    
                    # Score composite
                    objective_value = (
                        cluster_penalty + noise_penalty - silhouette_bonus
                    ) / target['weight']  # PondÃ©ration par prÃ©fÃ©rence
                    
                    if objective_value < best_objective:
                        best_objective = objective_value
                        best_target = target['name']
                else:
                    # Hors plage - pÃ©nalitÃ© basÃ©e sur distance
                    distance = min(abs(n_clusters - target['min']), abs(n_clusters - target['max']))
                    objective_value = 50.0 + distance * 2.0
                    
                    if objective_value < best_objective:
                        best_objective = objective_value
                        best_target = target['name']
            
            # Tracker les bons rÃ©sultats
            if silhouette > 0.1 and noise_ratio < 0.3:
                result_info = {
                    'eval': evaluation_count,
                    'params': f"MCS={min_cluster_size}, MS={min_samples}, Îµ={cluster_selection_epsilon:.2f}, {metric}, {cluster_selection_method}",
                    'clusters': n_clusters,
                    'noise': noise_ratio,
                    'silhouette': silhouette,
                    'objective': best_objective,
                    'target': best_target
                }
                best_results.append(result_info)
                
                # Garder seulement les 10 meilleurs
                best_results.sort(key=lambda x: x['objective'])
                best_results = best_results[:10]
            
            # Logs dÃ©taillÃ©s pour les bons rÃ©sultats
            if n_clusters <= 100 and (silhouette > 0.0 or evaluation_count % 10 == 0):
                print(f"[{evaluation_count:3d}/100] MCS={min_cluster_size:3d}, MS={min_samples:2d}, Îµ={cluster_selection_epsilon:.2f}, "
                      f"{metric[:4]}, {cluster_selection_method}: {n_clusters:2d} clusters, {noise_ratio:.1%} bruit, "
                      f"sil={silhouette:.3f}, obj={best_objective:.2f} ({best_target})")
            elif evaluation_count % 25 == 0:
                print(f"[{evaluation_count:3d}/100] Progress checkpoint - {len(best_results)} good results found so far...")
            
            return best_objective
            
        except Exception as e:
            return 150.0
    
    print(f"\nğŸš€ DÃ‰MARRAGE OPTIMISATION INTENSIVE (100 Ã©valuations)")
    print(f"â±ï¸  Temps estimÃ© : 45-60 minutes")
    print(f"ğŸ¯ Recherche du meilleur compromis clusters/silhouette/bruit")
    print("-" * 70)
    
    # Optimisation avec acquisition function amÃ©liorÃ©e
    result = gp_minimize(
        objective_intensive,
        space,
        n_calls=100,                    # 100 Ã‰VALUATIONS !
        n_initial_points=15,            # Plus de points initiaux alÃ©atoires
        acq_func='EI',                  # Expected Improvement
        acq_optimizer='sampling',       # Ã‰chantillonnage pour exploration
        random_state=42,
        verbose=False,
        n_jobs=1                        # Single thread pour stabilitÃ©
    )
    
    # RÃ©cupÃ©rer les meilleurs paramÃ¨tres
    best_mcs, best_ms, best_eps, best_metric, best_method = result.x
    
    print(f"\n" + "="*70)
    print(f"ğŸ† OPTIMISATION TERMINÃ‰E - {evaluation_count} Ã©valuations effectuÃ©es")
    print(f"ğŸ“Š {len(best_results)} rÃ©sultats de qualitÃ© identifiÃ©s")
    print(f"\nğŸ¥‡ MEILLEURS PARAMÃˆTRES GLOBAUX :")
    print(f"  min_cluster_size : {best_mcs}")
    print(f"  min_samples : {best_ms}")
    print(f"  cluster_selection_epsilon : {best_eps:.3f}")
    print(f"  metric : {best_metric}")
    print(f"  cluster_selection_method : {best_method}")
    print(f"  Score objectif : {result.fun:.4f}")
    
    # Afficher top 5 des meilleurs rÃ©sultats
    if best_results:
        print(f"\nğŸ… TOP 5 DES MEILLEURS RÃ‰SULTATS :")
        for i, res in enumerate(best_results[:5], 1):
            print(f"  {i}. {res['clusters']:2d} clusters, sil={res['silhouette']:.3f}, "
                  f"bruit={res['noise']:.1%} â†’ {res['target']}")
    
    # Utiliser les meilleurs paramÃ¨tres
    final_clusterer = hdbscan.HDBSCAN(
        min_cluster_size=best_mcs,
        min_samples=best_ms,
        cluster_selection_epsilon=best_eps,
        metric=best_metric,
        cluster_selection_method=best_method
    )
    
    optimization_used = "Optimisation intensive (100 Ã©valuations)"
    
except Exception as e:
    print(f"âŒ Optimisation Ã©chouÃ©e : {e}")
    print("ğŸ”„ Fallback vers paramÃ¨tres haute qualitÃ©")
    
    # ParamÃ¨tres de fallback optimisÃ©s
    final_clusterer = hdbscan.HDBSCAN(
        min_cluster_size=150,
        min_samples=25,
        cluster_selection_epsilon=1.2,
        metric='cosine',
        cluster_selection_method='eom'
    )
    
    optimization_used = "ParamÃ¨tres haute qualitÃ© (fallback)"

# CLUSTERING FINAL AVEC ANALYSE COMPLÃˆTE
print(f"\nğŸ¯ CLUSTERING FINAL HAUTE QUALITÃ‰")
print("="*50)

cluster_labels = final_clusterer.fit_predict(embeddings_2d)

# Statistiques complÃ¨tes
n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
n_noise = (cluster_labels == -1).sum()
noise_ratio = n_noise / len(cluster_labels)

print(f"ğŸ“Š RÃ‰SULTATS FINAUX ({optimization_used}) :")
print(f"  ğŸ¯ Clusters dÃ©tectÃ©s : {n_clusters}")
print(f"  ğŸ”‡ Points de bruit : {n_noise} ({noise_ratio:.2%})")

# Calculs de qualitÃ© Ã©tendus
silhouette_final = None
if n_clusters > 1:
    mask = cluster_labels != -1
    if mask.sum() > n_clusters:
        try:
            silhouette_final = silhouette_score(embeddings_2d[mask], cluster_labels[mask])
            print(f"  ğŸ“ˆ Score de silhouette : {silhouette_final:.4f}")
            
            if silhouette_final > 0.3:
                print(f"  ğŸ† EXCELLENT! Silhouette > 0.3")
            elif silhouette_final > 0.1:
                print(f"  âœ… BON! Silhouette > 0.1")
            elif silhouette_final > 0:
                print(f"  ğŸŸ¡ Acceptable - Silhouette positive")
            else:
                print(f"  âš ï¸  Silhouette nÃ©gative")
        except Exception as e:
            print(f"  âš ï¸  Score de silhouette : Non calculable ({e})")

# Ã‰valuation qualitative
print(f"\nğŸ–ï¸  Ã‰VALUATION QUALITÃ‰ :")
quality_score = 0

if 15 <= n_clusters <= 60:
    print(f"  âœ… Nombre de clusters optimal")
    quality_score += 3
elif 10 <= n_clusters <= 80:
    print(f"  ğŸŸ¡ Nombre de clusters acceptable")  
    quality_score += 2
else:
    print(f"  âš ï¸  Nombre de clusters Ã  optimiser")
    quality_score += 1

if noise_ratio < 0.05:
    print(f"  âœ… TrÃ¨s peu de bruit (<5%)")
    quality_score += 3
elif noise_ratio < 0.15:
    print(f"  ğŸŸ¡ Bruit acceptable (<15%)")
    quality_score += 2
else:
    print(f"  âš ï¸  Bruit Ã©levÃ© (>15%)")
    quality_score += 1

if silhouette_final and silhouette_final > 0.2:
    print(f"  âœ… Excellente sÃ©paration des clusters")
    quality_score += 3
elif silhouette_final and silhouette_final > 0:
    print(f"  ğŸŸ¡ SÃ©paration acceptable")
    quality_score += 2
else:
    print(f"  âš ï¸  AmÃ©lioration de sÃ©paration possible")
    quality_score += 1

print(f"  ğŸ¯ Score qualitÃ© global : {quality_score}/9")

# Ajouter au dataframe
df['cluster'] = cluster_labels
df['umap_x'] = embeddings_2d[:, 0]
df['umap_y'] = embeddings_2d[:, 1]
df['umap_z'] = embeddings_3d[:, 0]

# Statistiques de distribution avancÃ©es
print(f"\nğŸ“Š ANALYSE DE DISTRIBUTION :")
cluster_counts = pd.Series(cluster_labels).value_counts().sort_values(ascending=False)

print(f"  ğŸ“ˆ Plus gros cluster : {cluster_counts.iloc[0]} tickets")
print(f"  ğŸ“‰ Plus petit cluster : {cluster_counts.iloc[-1] if len(cluster_counts) > 1 else 'N/A'} tickets")
print(f"  ğŸ“Š MÃ©diane taille cluster : {cluster_counts.median():.0f} tickets")
print(f"  ğŸ“Š Ã‰cart-type taille : {cluster_counts.std():.0f} tickets")

# TOP 20 clusters
print(f"\nğŸ“Š TOP 20 CLUSTERS :")
for i, (cluster, count) in enumerate(cluster_counts.head(20).items()):
    if cluster == -1:
        print(f"  Bruit      : {count:5d} tickets ({count/len(df)*100:.1f}%)")
    else:
        print(f"  Cluster {cluster:2d} : {count:5d} tickets ({count/len(df)*100:.1f}%)")

if len(cluster_counts) > 20:
    remaining = len(cluster_counts) - 20
    remaining_tickets = cluster_counts.iloc[20:].sum()
    print(f"  ... + {remaining} autres clusters avec {remaining_tickets} tickets")

# Sauvegarde
print(f"\nğŸ’¾ SAUVEGARDE...")
output = dataiku.Dataset("incident_with_clusters_intensive")
output.write_with_schema(df)

print(f"\n" + "="*80)
print(f"ğŸ‰ CLUSTERING INTENSIF TERMINÃ‰ AVEC SUCCÃˆS!")
print(f"ğŸ“ Dataset : incident_with_clusters_intensive")
print(f"ğŸ† RÃ©sultat obtenu aprÃ¨s recherche exhaustive de 100 Ã©valuations")
print(f"ğŸ¯ PrÃªt pour l'analyse fine des causes et le dÃ©ploiement!")
print("="*80)
