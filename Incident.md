Parfait ! Maintenant adaptons le code pour acc√©der √† votre structure de dossiers. Voici le code corrig√© :

```python
# -*- coding: utf-8 -*-
import dataiku
import pandas as pd
import numpy as np
import os
import torch
from transformers import CamembertTokenizer, CamembertModel

# Read recipe inputs
dataset = dataiku.Dataset("incident_prepared_encode_categorical")
df = dataset.get_dataframe()

# Acc√©der au folder "models"
folder = dataiku.Folder("models")
folder_path = folder.get_path()

# Le mod√®le est dans le sous-dossier camembert_model
model_path = os.path.join(folder_path, "camembert_model")

print(f"Chemin du folder : {folder_path}")
print(f"Chemin du mod√®le : {model_path}")

# V√©rifier que les fichiers sont bien l√†
print("\nFichiers dans camembert_model:")
if os.path.exists(model_path):
    for file in os.listdir(model_path):
        print(f"  - {file}")
else:
    print("ERREUR: Le dossier camembert_model n'existe pas!")

# Charger le mod√®le et tokenizer
try:
    print("\nChargement du tokenizer...")
    tokenizer = CamembertTokenizer.from_pretrained(model_path, local_files_only=True)
    
    print("Chargement du mod√®le...")
    model = CamembertModel.from_pretrained(model_path, local_files_only=True)
    model.eval()  # Mettre en mode √©valuation
    
    print("‚úÖ Mod√®le charg√© avec succ√®s!")
    
except Exception as e:
    print(f"‚ùå Erreur lors du chargement : {e}")
    raise

# Fonction pour g√©n√©rer les embeddings
def generate_embeddings_batch(texts, batch_size=8):  # R√©duit √† 8 pour √©conomiser la m√©moire
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        
        try:
            # Tokenisation
            inputs = tokenizer(
                batch,
                padding=True,
                truncation=True,
                max_length=128,
                return_tensors='pt'
            )
            
            # G√©n√©ration des embeddings
            with torch.no_grad():
                outputs = model(**inputs)
                # Utiliser le token [CLS]
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            embeddings.extend(batch_embeddings)
            
            if i % 100 == 0:
                print(f"Trait√© {i}/{len(texts)} textes...")
                
        except Exception as e:
            print(f"Erreur sur le batch {i}: {e}")
            # Ajouter des embeddings vides en cas d'erreur
            embeddings.extend(np.zeros((len(batch), 768)))
    
    return np.array(embeddings)

# Pr√©parer les textes
print("\nPr√©paration des textes...")
texts = df['Notes de r√©solution'].fillna('').astype(str).tolist()

# Limiter pour test initial
print(f"\nNombre total de textes : {len(texts)}")

# Pour tester, commencer avec un √©chantillon
TEST_MODE = True  # Mettre √† False pour traiter tous les textes
if TEST_MODE:
    texts = texts[:100]  # Tester avec 100 textes d'abord
    print(f"Mode TEST : traitement de {len(texts)} textes seulement")

# G√©n√©rer les embeddings
print("\nG√©n√©ration des embeddings...")
embeddings = generate_embeddings_batch(texts)

print(f"‚úÖ Embeddings g√©n√©r√©s : shape {embeddings.shape}")

# Si en mode test, cr√©er un dataframe r√©duit
if TEST_MODE:
    df_output = df.head(100).copy()
else:
    df_output = df.copy()

# Ajouter les embeddings au dataframe
print("\nAjout des embeddings au dataframe...")
for i in range(min(embeddings.shape[1], 768)):  # CamemBERT = 768 dimensions
    df_output[f'embedding_{i}'] = embeddings[:, i]

print(f"Colonnes dans le dataframe final : {len(df_output.columns)}")

# Write recipe outputs
print("\nSauvegarde des r√©sultats...")
output = dataiku.Dataset("incident_prepared_embeddings_creation")
output.write_with_schema(df_output)

print("‚úÖ Termin√© avec succ√®s!")
```

## üîß Points importants :

1. **Mode TEST** : J'ai ajout√© un mode test qui traite seulement 100 textes. Une fois que √ßa marche, mettez `TEST_MODE = False`

2. **Gestion m√©moire** : J'ai r√©duit le batch_size √† 8 pour √©viter les probl√®mes de m√©moire

3. **Gestion d'erreurs** : Le code g√®re les erreurs potentielles lors du traitement

4. **V√©rification des fichiers** : Le code liste les fichiers pour v√©rifier qu'ils sont bien l√†

## üö® Si vous avez une erreur :

1. **V√©rifiez les logs** pour voir quels fichiers sont d√©tect√©s
2. **V√©rifiez les permissions** sur le folder
3. **Essayez avec un batch_size encore plus petit** (4 ou 2) si probl√®me de m√©moire

Lancez d'abord en mode TEST pour v√©rifier que tout fonctionne !
